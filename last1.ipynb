{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, sampler\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC, SMOTEN, RandomOverSampler, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler, EditedNearestNeighbours, RepeatedEditedNearestNeighbours, AllKNN, CondensedNearestNeighbour, OneSidedSelection, NeighbourhoodCleaningRule\n",
    "\n",
    "import shap\n",
    "import catboost\n",
    "from catboost import Pool, cv\n",
    "\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 248 entries, 0 to 247\n",
      "Data columns (total 18 columns):\n",
      " #   Column  Non-Null Count  Dtype   \n",
      "---  ------  --------------  -----   \n",
      " 0   id      248 non-null    object  \n",
      " 1   trait   248 non-null    category\n",
      " 2   SNP_01  248 non-null    category\n",
      " 3   SNP_02  248 non-null    category\n",
      " 4   SNP_03  248 non-null    category\n",
      " 5   SNP_04  248 non-null    category\n",
      " 6   SNP_05  248 non-null    category\n",
      " 7   SNP_06  248 non-null    category\n",
      " 8   SNP_07  248 non-null    category\n",
      " 9   SNP_08  248 non-null    category\n",
      " 10  SNP_09  248 non-null    category\n",
      " 11  SNP_10  248 non-null    category\n",
      " 12  SNP_11  248 non-null    category\n",
      " 13  SNP_12  248 non-null    category\n",
      " 14  SNP_13  248 non-null    category\n",
      " 15  SNP_14  248 non-null    category\n",
      " 16  SNP_15  248 non-null    category\n",
      " 17  class   248 non-null    object  \n",
      "dtypes: category(16), object(2)\n",
      "memory usage: 9.9+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 175 entries, 0 to 174\n",
      "Data columns (total 17 columns):\n",
      " #   Column  Non-Null Count  Dtype   \n",
      "---  ------  --------------  -----   \n",
      " 0   id      175 non-null    object  \n",
      " 1   trait   175 non-null    category\n",
      " 2   SNP_01  175 non-null    category\n",
      " 3   SNP_02  175 non-null    category\n",
      " 4   SNP_03  175 non-null    category\n",
      " 5   SNP_04  175 non-null    category\n",
      " 6   SNP_05  175 non-null    category\n",
      " 7   SNP_06  175 non-null    category\n",
      " 8   SNP_07  175 non-null    category\n",
      " 9   SNP_08  175 non-null    category\n",
      " 10  SNP_09  175 non-null    category\n",
      " 11  SNP_10  175 non-null    category\n",
      " 12  SNP_11  175 non-null    category\n",
      " 13  SNP_12  175 non-null    category\n",
      " 14  SNP_13  175 non-null    category\n",
      " 15  SNP_14  175 non-null    category\n",
      " 16  SNP_15  175 non-null    category\n",
      "dtypes: category(16), object(1)\n",
      "memory usage: 6.3+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"./data/train.csv\").drop(columns=['father', 'mother', 'gender'])\n",
    "train.drop_duplicates(subset=train.columns.tolist()[5:20], inplace=True, ignore_index=True)\n",
    "test = pd.read_csv(\"./data/test.csv\").drop(columns=['father', 'mother', 'gender'])\n",
    "\n",
    "train.iloc[:, 1:-1] = train.iloc[:, 1:-1].astype('category')\n",
    "test.iloc[:, 1:] = test.iloc[:, 1:].astype('category')\n",
    "\n",
    "answer = np.zeros(len(test)) - 1\n",
    "\n",
    "train.info(), test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 1869.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 248 entries, 0 to 247\n",
      "Data columns (total 18 columns):\n",
      " #   Column  Non-Null Count  Dtype   \n",
      "---  ------  --------------  -----   \n",
      " 0   id      248 non-null    object  \n",
      " 1   trait   248 non-null    category\n",
      " 2   SNP_01  248 non-null    category\n",
      " 3   SNP_02  248 non-null    category\n",
      " 4   SNP_03  248 non-null    category\n",
      " 5   SNP_04  248 non-null    category\n",
      " 6   SNP_05  248 non-null    category\n",
      " 7   SNP_06  248 non-null    category\n",
      " 8   SNP_07  248 non-null    category\n",
      " 9   SNP_08  248 non-null    category\n",
      " 10  SNP_09  248 non-null    category\n",
      " 11  SNP_10  248 non-null    category\n",
      " 12  SNP_11  248 non-null    category\n",
      " 13  SNP_12  248 non-null    category\n",
      " 14  SNP_13  248 non-null    category\n",
      " 15  SNP_14  248 non-null    category\n",
      " 16  SNP_15  248 non-null    category\n",
      " 17  class   248 non-null    object  \n",
      "dtypes: category(16), object(2)\n",
      "memory usage: 9.9+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 175 entries, 0 to 174\n",
      "Data columns (total 17 columns):\n",
      " #   Column  Non-Null Count  Dtype   \n",
      "---  ------  --------------  -----   \n",
      " 0   id      175 non-null    object  \n",
      " 1   trait   175 non-null    category\n",
      " 2   SNP_01  175 non-null    category\n",
      " 3   SNP_02  175 non-null    category\n",
      " 4   SNP_03  175 non-null    category\n",
      " 5   SNP_04  175 non-null    category\n",
      " 6   SNP_05  175 non-null    category\n",
      " 7   SNP_06  175 non-null    category\n",
      " 8   SNP_07  175 non-null    category\n",
      " 9   SNP_08  175 non-null    category\n",
      " 10  SNP_09  175 non-null    category\n",
      " 11  SNP_10  175 non-null    category\n",
      " 12  SNP_11  175 non-null    category\n",
      " 13  SNP_12  175 non-null    category\n",
      " 14  SNP_13  175 non-null    category\n",
      " 15  SNP_14  175 non-null    category\n",
      " 16  SNP_15  175 non-null    category\n",
      "dtypes: category(16), object(1)\n",
      "memory usage: 6.3+ KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text 형태의 categorical 변수들을 숫자형태로 변경\n",
    "\n",
    "for i in tqdm(range(1, 15+1)) :\n",
    "    target = str(i) if i >= 10 else \"0\"+str(i)\n",
    "    try :   \n",
    "        cols = sorted(train[f\"SNP_{target}\"].unique().tolist())  \n",
    "        train[f\"SNP_{target}\"] = train[f\"SNP_{target}\"].map(lambda x : 0 if x==cols[0] else (1 if x==cols[1] else 2))\n",
    "        test[f\"SNP_{target}\"] = test[f\"SNP_{target}\"].map(lambda x : 0 if x==cols[0] else (1 if x==cols[1] else 2))\n",
    "    except :\n",
    "        continue\n",
    "\n",
    "train.info(), test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed=0\n",
    "strategy1 = {0 : 40, 1 : 70, 2 : 50}\n",
    "\n",
    "under1 = RandomUnderSampler(sampling_strategy=strategy1, random_state=random_seed)\n",
    "under2 = EditedNearestNeighbours()\n",
    "under3 = RepeatedEditedNearestNeighbours()\n",
    "under4 = AllKNN()\n",
    "under5 = CondensedNearestNeighbour(random_state=random_seed)\n",
    "under6 = OneSidedSelection(random_state=random_seed)\n",
    "under7 = NeighbourhoodCleaningRule()\n",
    "\n",
    "X, y = train.iloc[:, 1:-1], train['class'].map(lambda x : 0 if x=='A' else (1 if x=='B' else 2)).values\n",
    "\n",
    "X1, y1 = under1.fit_resample(X, y)\n",
    "X2, y2 = under2.fit_resample(X, y)\n",
    "X3, y3 = under3.fit_resample(X, y)\n",
    "X4, y4 = under4.fit_resample(X, y)\n",
    "X5, y5 = under5.fit_resample(X, y)\n",
    "X6, y6 = under6.fit_resample(X, y)\n",
    "X7, y7 = under7.fit_resample(X, y)\n",
    "\n",
    "strategy2 = {0 : 100, 1 : 120, 2 : 110}\n",
    "\n",
    "over1 = SMOTEN(sampling_strategy=strategy2, random_state=random_seed)\n",
    "over2 = SMOTE(sampling_strategy=strategy2, random_state=random_seed)\n",
    "over3 = RandomOverSampler(sampling_strategy=strategy2, random_state=random_seed)\n",
    "\n",
    "X8, y8 = over1.fit_resample(X, y)\n",
    "X9, y9 = over2.fit_resample(X, y)\n",
    "X10, y10 = over3.fit_resample(X, y)\n",
    "\n",
    "df_train = pd.concat([X1, X2, X3, X4, X5, X6, X7, X8, X9, X10], ignore_index=True)\n",
    "df_train['class'] = list(y1)+list(y2)+list(y3)+list(y4)+list(y5)+list(y6)+list(y7)+list(y8)+list(y9)+list(y10)\n",
    "\n",
    "df = df_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed= 10\n",
    "strategy1 = {0 : 40, 1 : 70, 2 : 50}\n",
    "\n",
    "under1 = RandomUnderSampler(sampling_strategy=strategy1, random_state=random_seed)\n",
    "under2 = EditedNearestNeighbours()\n",
    "under3 = RepeatedEditedNearestNeighbours()\n",
    "under4 = AllKNN()\n",
    "under5 = CondensedNearestNeighbour(random_state=random_seed)\n",
    "under6 = OneSidedSelection(random_state=random_seed)\n",
    "under7 = NeighbourhoodCleaningRule()\n",
    "\n",
    "X, y = train.iloc[:, 1:-1], train['class'].map(lambda x : 0 if x=='A' else (1 if x=='B' else 2)).values\n",
    "\n",
    "X1, y1 = under1.fit_resample(X, y)\n",
    "X2, y2 = under2.fit_resample(X, y)\n",
    "X3, y3 = under3.fit_resample(X, y)\n",
    "X4, y4 = under4.fit_resample(X, y)\n",
    "X5, y5 = under5.fit_resample(X, y)\n",
    "X6, y6 = under6.fit_resample(X, y)\n",
    "X7, y7 = under7.fit_resample(X, y)\n",
    "\n",
    "strategy2 = {0 : 100, 1 : 120, 2 : 110}\n",
    "\n",
    "over1 = SMOTEN(sampling_strategy=strategy2, random_state=random_seed)\n",
    "over2 = SMOTE(sampling_strategy=strategy2, random_state=random_seed)\n",
    "over3 = RandomOverSampler(sampling_strategy=strategy2, random_state=random_seed)\n",
    "\n",
    "X8, y8 = over1.fit_resample(X, y)\n",
    "X9, y9 = over2.fit_resample(X, y)\n",
    "X10, y10 = over3.fit_resample(X, y)\n",
    "\n",
    "df_train = pd.concat([X1, X2, X3, X4, X5, X6, X7, X8, X9, X10], ignore_index=True)\n",
    "df_train['class'] = list(y1)+list(y2)+list(y3)+list(y4)+list(y5)+list(y6)+list(y7)+list(y8)+list(y9)+list(y10)\n",
    "\n",
    "df = pd.concat([df, df_train], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed= 100\n",
    "strategy1 = {0 : 40, 1 : 70, 2 : 50}\n",
    "\n",
    "under1 = RandomUnderSampler(sampling_strategy=strategy1, random_state=random_seed)\n",
    "under2 = EditedNearestNeighbours()\n",
    "under3 = RepeatedEditedNearestNeighbours()\n",
    "under4 = AllKNN()\n",
    "under5 = CondensedNearestNeighbour(random_state=random_seed)\n",
    "under6 = OneSidedSelection(random_state=random_seed)\n",
    "under7 = NeighbourhoodCleaningRule()\n",
    "\n",
    "X, y = train.iloc[:, 1:-1], train['class'].map(lambda x : 0 if x=='A' else (1 if x=='B' else 2)).values\n",
    "\n",
    "X1, y1 = under1.fit_resample(X, y)\n",
    "X2, y2 = under2.fit_resample(X, y)\n",
    "X3, y3 = under3.fit_resample(X, y)\n",
    "X4, y4 = under4.fit_resample(X, y)\n",
    "X5, y5 = under5.fit_resample(X, y)\n",
    "X6, y6 = under6.fit_resample(X, y)\n",
    "X7, y7 = under7.fit_resample(X, y)\n",
    "\n",
    "strategy2 = {0 : 100, 1 : 120, 2 : 110}\n",
    "\n",
    "over1 = SMOTEN(sampling_strategy=strategy2, random_state=random_seed)\n",
    "over2 = SMOTE(sampling_strategy=strategy2, random_state=random_seed)\n",
    "over3 = RandomOverSampler(sampling_strategy=strategy2, random_state=random_seed)\n",
    "\n",
    "X8, y8 = over1.fit_resample(X, y)\n",
    "X9, y9 = over2.fit_resample(X, y)\n",
    "X10, y10 = over3.fit_resample(X, y)\n",
    "\n",
    "df_train = pd.concat([X1, X2, X3, X4, X5, X6, X7, X8, X9, X10], ignore_index=True)\n",
    "df_train['class'] = list(y1)+list(y2)+list(y3)+list(y4)+list(y5)+list(y6)+list(y7)+list(y8)+list(y9)+list(y10)\n",
    "\n",
    "df = pd.concat([df, df_train], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trait</th>\n",
       "      <th>SNP_01</th>\n",
       "      <th>SNP_02</th>\n",
       "      <th>SNP_03</th>\n",
       "      <th>SNP_04</th>\n",
       "      <th>SNP_05</th>\n",
       "      <th>SNP_06</th>\n",
       "      <th>SNP_07</th>\n",
       "      <th>SNP_08</th>\n",
       "      <th>SNP_09</th>\n",
       "      <th>SNP_10</th>\n",
       "      <th>SNP_11</th>\n",
       "      <th>SNP_12</th>\n",
       "      <th>SNP_13</th>\n",
       "      <th>SNP_14</th>\n",
       "      <th>SNP_15</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>477 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    trait SNP_01 SNP_02 SNP_03 SNP_04 SNP_05 SNP_06 SNP_07 SNP_08 SNP_09  \\\n",
       "0       1      1      2      0      2      0      2      2      0      2   \n",
       "1       1      1      2      0      1      1      1      1      0      1   \n",
       "2       1      0      2      0      1      0      2      2      0      1   \n",
       "3       1      0      2      0      2      0      2      2      0      1   \n",
       "4       1      0      2      0      1      1      2      1      0      0   \n",
       "..    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "472     2      1      1      0      0      1      0      0      0      0   \n",
       "473     2      1      0      0      0      1      0      0      1      0   \n",
       "474     2      1      1      1      0      1      0      0      0      0   \n",
       "475     2      1      1      1      0      0      1      0      1      0   \n",
       "476     2      2      1      0      0      0      1      1      1      0   \n",
       "\n",
       "    SNP_10 SNP_11 SNP_12 SNP_13 SNP_14 SNP_15  class  \n",
       "0        0      2      1      2      1      1      0  \n",
       "1        0      2      1      2      1      0      0  \n",
       "2        1      1      1      2      1      2      0  \n",
       "3        1      2      1      2      1      2      0  \n",
       "4        0      2      2      2      2      2      0  \n",
       "..     ...    ...    ...    ...    ...    ...    ...  \n",
       "472      1      0      0      1      0      1      2  \n",
       "473      0      0      0      1      0      1      2  \n",
       "474      1      1      0      2      1      1      2  \n",
       "475      1      0      0      2      0      0      2  \n",
       "476      2      1      0      0      0      0      2  \n",
       "\n",
       "[477 rows x 17 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates(ignore_index=True, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.iloc[:, :-1].to_numpy(), df['class']\n",
    "\n",
    "# Load the data\n",
    "data = torch.from_numpy(X)\n",
    "\n",
    "# Preprocess the data by one-hot encoding the categories\n",
    "one_hot_data_01 = F.one_hot(data[:,:1]-1, num_classes=2).view(len(X), 2).float()\n",
    "one_hot_data_02 = F.one_hot(data[:,1:], num_classes=3).view(len(X), 3*data[:,1:].size(1)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([477, 2]), torch.Size([477, 45]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_data_01.shape, one_hot_data_02.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([477, 47])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_data = torch.concat([one_hot_data_01, one_hot_data_02], axis=1)\n",
    "one_hot_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/100000], D_loss: 3.8957, G_loss: 0.0431\n",
      "Epoch [400/100000], D_loss: 6.1195, G_loss: 0.0075\n",
      "Epoch [600/100000], D_loss: 6.0832, G_loss: 0.0074\n",
      "Epoch [800/100000], D_loss: 6.1410, G_loss: 0.0071\n",
      "Epoch [1000/100000], D_loss: 6.2362, G_loss: 0.0083\n",
      "Epoch [1200/100000], D_loss: 6.1514, G_loss: 0.0065\n",
      "Epoch [1400/100000], D_loss: 6.0868, G_loss: 0.0068\n",
      "Epoch [1600/100000], D_loss: 6.1437, G_loss: 0.0073\n",
      "Epoch [1800/100000], D_loss: 6.1423, G_loss: 0.0068\n",
      "Epoch [2000/100000], D_loss: 6.2221, G_loss: 0.0074\n",
      "Epoch [2200/100000], D_loss: 6.2058, G_loss: 0.0074\n",
      "Epoch [2400/100000], D_loss: 6.1394, G_loss: 0.0068\n",
      "Epoch [2600/100000], D_loss: 6.2495, G_loss: 0.0072\n",
      "Epoch [2800/100000], D_loss: 6.2265, G_loss: 0.0072\n",
      "Epoch [3000/100000], D_loss: 6.1758, G_loss: 0.0075\n",
      "Epoch [3200/100000], D_loss: 6.1714, G_loss: 0.0073\n",
      "Epoch [3400/100000], D_loss: 6.1211, G_loss: 0.0076\n",
      "Epoch [3600/100000], D_loss: 6.2072, G_loss: 0.0065\n",
      "Epoch [3800/100000], D_loss: 6.1053, G_loss: 0.0078\n",
      "Epoch [4000/100000], D_loss: 6.1717, G_loss: 0.0074\n",
      "Epoch [4200/100000], D_loss: 6.0909, G_loss: 0.0068\n",
      "Epoch [4400/100000], D_loss: 6.2011, G_loss: 0.0075\n",
      "Epoch [4600/100000], D_loss: 6.1156, G_loss: 0.0068\n",
      "Epoch [4800/100000], D_loss: 6.2600, G_loss: 0.0068\n",
      "Epoch [5000/100000], D_loss: 6.0436, G_loss: 0.0073\n",
      "Epoch [5200/100000], D_loss: 6.2113, G_loss: 0.0068\n",
      "Epoch [5400/100000], D_loss: 6.0389, G_loss: 0.0072\n",
      "Epoch [5600/100000], D_loss: 6.1390, G_loss: 0.0071\n",
      "Epoch [5800/100000], D_loss: 6.3573, G_loss: 0.0068\n",
      "Epoch [6000/100000], D_loss: 6.1437, G_loss: 0.0076\n",
      "Epoch [6200/100000], D_loss: 6.0761, G_loss: 0.0067\n",
      "Epoch [6400/100000], D_loss: 6.1865, G_loss: 0.0067\n",
      "Epoch [6600/100000], D_loss: 6.2137, G_loss: 0.0071\n",
      "Epoch [6800/100000], D_loss: 6.0952, G_loss: 0.0074\n",
      "Epoch [7000/100000], D_loss: 6.1384, G_loss: 0.0074\n",
      "Epoch [7200/100000], D_loss: 6.1985, G_loss: 0.0077\n",
      "Epoch [7400/100000], D_loss: 6.1135, G_loss: 0.0065\n",
      "Epoch [7600/100000], D_loss: 6.1194, G_loss: 0.0066\n",
      "Epoch [7800/100000], D_loss: 6.1465, G_loss: 0.0071\n",
      "Epoch [8000/100000], D_loss: 6.0881, G_loss: 0.0067\n",
      "Epoch [8200/100000], D_loss: 6.0600, G_loss: 0.0073\n",
      "Epoch [8400/100000], D_loss: 6.1037, G_loss: 0.0072\n",
      "Epoch [8600/100000], D_loss: 6.3172, G_loss: 0.0070\n",
      "Epoch [8800/100000], D_loss: 6.1884, G_loss: 0.0075\n",
      "Epoch [9000/100000], D_loss: 6.1771, G_loss: 0.0074\n",
      "Epoch [9200/100000], D_loss: 6.0876, G_loss: 0.0081\n",
      "Epoch [9400/100000], D_loss: 6.1218, G_loss: 0.0069\n",
      "Epoch [9600/100000], D_loss: 6.2546, G_loss: 0.0068\n",
      "Epoch [9800/100000], D_loss: 6.1859, G_loss: 0.0072\n",
      "Epoch [10000/100000], D_loss: 6.0813, G_loss: 0.0068\n",
      "Epoch [10200/100000], D_loss: 6.1504, G_loss: 0.0071\n",
      "Epoch [10400/100000], D_loss: 6.2578, G_loss: 0.0077\n",
      "Epoch [10600/100000], D_loss: 6.2558, G_loss: 0.0073\n",
      "Epoch [10800/100000], D_loss: 6.2134, G_loss: 0.0064\n",
      "Epoch [11000/100000], D_loss: 6.1460, G_loss: 0.0071\n",
      "Epoch [11200/100000], D_loss: 6.1398, G_loss: 0.0071\n",
      "Epoch [11400/100000], D_loss: 6.2185, G_loss: 0.0076\n",
      "Epoch [11600/100000], D_loss: 6.1201, G_loss: 0.0071\n",
      "Epoch [11800/100000], D_loss: 6.1226, G_loss: 0.0067\n",
      "Epoch [12000/100000], D_loss: 6.1908, G_loss: 0.0075\n",
      "Epoch [12200/100000], D_loss: 6.2036, G_loss: 0.0075\n",
      "Epoch [12400/100000], D_loss: 6.0634, G_loss: 0.0073\n",
      "Epoch [12600/100000], D_loss: 6.1562, G_loss: 0.0070\n",
      "Epoch [12800/100000], D_loss: 6.1391, G_loss: 0.0075\n",
      "Epoch [13000/100000], D_loss: 6.2339, G_loss: 0.0071\n",
      "Epoch [13200/100000], D_loss: 6.1349, G_loss: 0.0067\n",
      "Epoch [13400/100000], D_loss: 6.1959, G_loss: 0.0071\n",
      "Epoch [13600/100000], D_loss: 6.0336, G_loss: 0.0070\n",
      "Epoch [13800/100000], D_loss: 6.1559, G_loss: 0.0068\n",
      "Epoch [14000/100000], D_loss: 6.0995, G_loss: 0.0070\n",
      "Epoch [14200/100000], D_loss: 6.1076, G_loss: 0.0065\n",
      "Epoch [14400/100000], D_loss: 6.1569, G_loss: 0.0074\n",
      "Epoch [14600/100000], D_loss: 6.1015, G_loss: 0.0076\n",
      "Epoch [14800/100000], D_loss: 6.1449, G_loss: 0.0073\n",
      "Epoch [15000/100000], D_loss: 6.1450, G_loss: 0.0076\n",
      "Epoch [15200/100000], D_loss: 6.2260, G_loss: 0.0073\n",
      "Epoch [15400/100000], D_loss: 6.1428, G_loss: 0.0077\n",
      "Epoch [15600/100000], D_loss: 6.1639, G_loss: 0.0074\n",
      "Epoch [15800/100000], D_loss: 6.1721, G_loss: 0.0073\n",
      "Epoch [16000/100000], D_loss: 6.1370, G_loss: 0.0070\n",
      "Epoch [16200/100000], D_loss: 6.1011, G_loss: 0.0073\n",
      "Epoch [16400/100000], D_loss: 6.1544, G_loss: 0.0070\n",
      "Epoch [16600/100000], D_loss: 6.3118, G_loss: 0.0076\n",
      "Epoch [16800/100000], D_loss: 6.2346, G_loss: 0.0066\n",
      "Epoch [17000/100000], D_loss: 6.1599, G_loss: 0.0069\n",
      "Epoch [17200/100000], D_loss: 6.3061, G_loss: 0.0070\n",
      "Epoch [17400/100000], D_loss: 6.1997, G_loss: 0.0079\n",
      "Epoch [17600/100000], D_loss: 6.1484, G_loss: 0.0072\n",
      "Epoch [17800/100000], D_loss: 6.1623, G_loss: 0.0071\n",
      "Epoch [18000/100000], D_loss: 6.2183, G_loss: 0.0071\n",
      "Epoch [18200/100000], D_loss: 6.1038, G_loss: 0.0069\n",
      "Epoch [18400/100000], D_loss: 6.2029, G_loss: 0.0072\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [56], line 101\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39m# Compute loss and backpropagate\u001b[39;00m\n\u001b[1;32m    100\u001b[0m g_loss \u001b[39m=\u001b[39m criterion(fake_score, torch\u001b[39m.\u001b[39mones_like(fake_score))\n\u001b[0;32m--> 101\u001b[0m g_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    102\u001b[0m g_optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    103\u001b[0m g_scheduler\u001b[39m.\u001b[39mstep(g_loss)  \u001b[39m# Update learning rate\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "latent_size = 3\n",
    "hidden_size = 64\n",
    "num_epochs = 100000\n",
    "batch_size = len(one_hot_data)\n",
    "sample_size = 100\n",
    "\n",
    "# Custom dataloader\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "# Create the dataset\n",
    "data = one_hot_data # Replace this with your own data\n",
    "dataset = CustomDataset(data)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(47, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 47)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize models and move to device\n",
    "D = Discriminator().to(device)\n",
    "G = Generator().to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "d_optimizer = torch.optim.Adam(D.parameters(), lr=0.0002)\n",
    "g_optimizer = torch.optim.Adam(G.parameters(), lr=0.0001999)\n",
    "\n",
    "# Scheduler\n",
    "d_scheduler = lr_scheduler.ReduceLROnPlateau(d_optimizer, 'min')\n",
    "g_scheduler = lr_scheduler.ReduceLROnPlateau(g_optimizer, 'min')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(dataloader):\n",
    "        # 1. Train the discriminator\n",
    "        D.zero_grad()\n",
    "        real_data = data.to(device)\n",
    "        real_score = D(real_data)\n",
    "        real_loss = criterion(real_score, torch.ones_like(real_score))\n",
    "\n",
    "        # Generate fake data\n",
    "        latent = torch.randn(batch_size, latent_size).to(device)\n",
    "        fake_data = G(latent)\n",
    "        fake_score = D(fake_data)\n",
    "        fake_loss = criterion(fake_score, torch.zeros_like(fake_score))\n",
    "\n",
    "        # Compute loss and backpropagate\n",
    "        d_loss = real_loss + fake_loss\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        d_scheduler.step(d_loss)  # Update learning rate\n",
    "\n",
    "        # 2. Train the generator\n",
    "        G.zero_grad()\n",
    "        latent = torch.randn(batch_size, latent_size).to(device)\n",
    "        fake_data = G(latent)\n",
    "        fake_score = D(fake_data)\n",
    "\n",
    "        # Compute loss and backpropagate\n",
    "        g_loss = criterion(fake_score, torch.ones_like(fake_score))\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        g_scheduler.step(g_loss)  # Update learning rate\n",
    "\n",
    "    # Print loss every 10 epochs\n",
    "    if (epoch+1) % 200 == 0:\n",
    "        print('Epoch [{}/{}], D_loss: {:.4f}, G_loss: {:.4f}'.format(epoch+1, num_epochs, d_loss.item(), g_loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(recon_x.view(16,3), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.argmax(x.view(16,3), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load the data\n",
    "data = torch.from_numpy(X)\n",
    "\n",
    "# Preprocess the data by one-hot encoding the categories\n",
    "one_hot_data = torch.zeros(data.size(0), data.max()+1)\n",
    "one_hot_data[torch.arange(data.size(0)), data] = 1\n",
    "\n",
    "# Define the generator network\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, 32)\n",
    "        self.fc2 = nn.Linear(32, output_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        h1 = torch.relu(self.fc1(z))\n",
    "        return self.fc2(h1)\n",
    "\n",
    "# Define the discriminator network\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, 32)\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1 = torch.relu(self.fc1(x))\n",
    "        return self.fc2(h1)\n",
    "\n",
    "# Instantiate the generator and discriminator networks\n",
    "generator = Generator(input_dim=100, output_dim=one_hot_data.size(1))\n",
    "discriminator = Discriminator(input_dim=one_hot_data.size(1))\n",
    "\n",
    "# Define the loss function and optimizers\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=1e-3)\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=1e-3)\n",
    "\n",
    "# Train the GAN\n",
    "for epoch in range(10):\n",
    "    for i, real_data in enumerate(one_hot_data):\n",
    "        # Generate fake data\n",
    "        noise = torch.randn(100)\n",
    "        fake_data = generator(noise)\n",
    "\n",
    "        # Train the discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        loss_real = F.binary_cross_entropy(discriminator(real_data), torch.ones_like(discriminator(real_data)))\n",
    "        loss_fake = F.binary_cross_entropy(discriminator(fake_data), torch.zeros_like(discriminator(fake_data)))\n",
    "        loss_D = (loss_real + loss_fake) / 2\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Train the generator\n",
    "        optimizer_G.zero_grad()\n",
    "        loss_G = F.binary_cross_entropy(discriminator(fake_data), torch.ones_like(discriminator(fake_data)))\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "# Use the generator to generate new variables\n",
    "new_variables = generator(torch.randn(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Load the data\n",
    "data = np.load('categorical_data.npy')\n",
    "\n",
    "# Preprocess the data by one-hot encoding the categories\n",
    "one_hot_data = np.zeros((data.size, data.max()+1))\n",
    "one_hot_data[np.arange(data.size), data] = 1\n",
    "\n",
    "# Use t-SNE to reduce the dimensionality of the data\n",
    "tsne = TSNE(n_components=2)\n",
    "new_variables = tsne.fit_transform(one_hot_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
