{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, sampler\n",
    "\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC, SMOTEN, RandomOverSampler, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler, EditedNearestNeighbours, RepeatedEditedNearestNeighbours, AllKNN, CondensedNearestNeighbour, OneSidedSelection, NeighbourhoodCleaningRule\n",
    "\n",
    "import shap\n",
    "import catboost\n",
    "from catboost import Pool, cv\n",
    "\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 248 entries, 0 to 247\n",
      "Data columns (total 18 columns):\n",
      " #   Column  Non-Null Count  Dtype   \n",
      "---  ------  --------------  -----   \n",
      " 0   id      248 non-null    object  \n",
      " 1   trait   248 non-null    category\n",
      " 2   SNP_01  248 non-null    category\n",
      " 3   SNP_02  248 non-null    category\n",
      " 4   SNP_03  248 non-null    category\n",
      " 5   SNP_04  248 non-null    category\n",
      " 6   SNP_05  248 non-null    category\n",
      " 7   SNP_06  248 non-null    category\n",
      " 8   SNP_07  248 non-null    category\n",
      " 9   SNP_08  248 non-null    category\n",
      " 10  SNP_09  248 non-null    category\n",
      " 11  SNP_10  248 non-null    category\n",
      " 12  SNP_11  248 non-null    category\n",
      " 13  SNP_12  248 non-null    category\n",
      " 14  SNP_13  248 non-null    category\n",
      " 15  SNP_14  248 non-null    category\n",
      " 16  SNP_15  248 non-null    category\n",
      " 17  class   248 non-null    object  \n",
      "dtypes: category(16), object(2)\n",
      "memory usage: 9.9+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 175 entries, 0 to 174\n",
      "Data columns (total 17 columns):\n",
      " #   Column  Non-Null Count  Dtype   \n",
      "---  ------  --------------  -----   \n",
      " 0   id      175 non-null    object  \n",
      " 1   trait   175 non-null    category\n",
      " 2   SNP_01  175 non-null    category\n",
      " 3   SNP_02  175 non-null    category\n",
      " 4   SNP_03  175 non-null    category\n",
      " 5   SNP_04  175 non-null    category\n",
      " 6   SNP_05  175 non-null    category\n",
      " 7   SNP_06  175 non-null    category\n",
      " 8   SNP_07  175 non-null    category\n",
      " 9   SNP_08  175 non-null    category\n",
      " 10  SNP_09  175 non-null    category\n",
      " 11  SNP_10  175 non-null    category\n",
      " 12  SNP_11  175 non-null    category\n",
      " 13  SNP_12  175 non-null    category\n",
      " 14  SNP_13  175 non-null    category\n",
      " 15  SNP_14  175 non-null    category\n",
      " 16  SNP_15  175 non-null    category\n",
      "dtypes: category(16), object(1)\n",
      "memory usage: 6.3+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"./data/train.csv\").drop(columns=['father', 'mother', 'gender'])\n",
    "train.drop_duplicates(subset=train.columns.tolist()[5:20], inplace=True, ignore_index=True)\n",
    "test = pd.read_csv(\"./data/test.csv\").drop(columns=['father', 'mother', 'gender'])\n",
    "\n",
    "train.iloc[:, 1:-1] = train.iloc[:, 1:-1].astype('category')\n",
    "test.iloc[:, 1:] = test.iloc[:, 1:].astype('category')\n",
    "\n",
    "answer = np.zeros(len(test)) - 1\n",
    "\n",
    "train.info(), test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b742ead2584eb78e87897840ffa57e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 248 entries, 0 to 247\n",
      "Data columns (total 18 columns):\n",
      " #   Column  Non-Null Count  Dtype   \n",
      "---  ------  --------------  -----   \n",
      " 0   id      248 non-null    object  \n",
      " 1   trait   248 non-null    category\n",
      " 2   SNP_01  248 non-null    category\n",
      " 3   SNP_02  248 non-null    category\n",
      " 4   SNP_03  248 non-null    category\n",
      " 5   SNP_04  248 non-null    category\n",
      " 6   SNP_05  248 non-null    category\n",
      " 7   SNP_06  248 non-null    category\n",
      " 8   SNP_07  248 non-null    category\n",
      " 9   SNP_08  248 non-null    category\n",
      " 10  SNP_09  248 non-null    category\n",
      " 11  SNP_10  248 non-null    category\n",
      " 12  SNP_11  248 non-null    category\n",
      " 13  SNP_12  248 non-null    category\n",
      " 14  SNP_13  248 non-null    category\n",
      " 15  SNP_14  248 non-null    category\n",
      " 16  SNP_15  248 non-null    category\n",
      " 17  class   248 non-null    object  \n",
      "dtypes: category(16), object(2)\n",
      "memory usage: 9.9+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 175 entries, 0 to 174\n",
      "Data columns (total 17 columns):\n",
      " #   Column  Non-Null Count  Dtype   \n",
      "---  ------  --------------  -----   \n",
      " 0   id      175 non-null    object  \n",
      " 1   trait   175 non-null    category\n",
      " 2   SNP_01  175 non-null    category\n",
      " 3   SNP_02  175 non-null    category\n",
      " 4   SNP_03  175 non-null    category\n",
      " 5   SNP_04  175 non-null    category\n",
      " 6   SNP_05  175 non-null    category\n",
      " 7   SNP_06  175 non-null    category\n",
      " 8   SNP_07  175 non-null    category\n",
      " 9   SNP_08  175 non-null    category\n",
      " 10  SNP_09  175 non-null    category\n",
      " 11  SNP_10  175 non-null    category\n",
      " 12  SNP_11  175 non-null    category\n",
      " 13  SNP_12  175 non-null    category\n",
      " 14  SNP_13  175 non-null    category\n",
      " 15  SNP_14  175 non-null    category\n",
      " 16  SNP_15  175 non-null    category\n",
      "dtypes: category(16), object(1)\n",
      "memory usage: 6.3+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text 형태의 categorical 변수들을 숫자형태로 변경\n",
    "\n",
    "for i in tqdm(range(1, 15+1)) :\n",
    "    target = str(i) if i >= 10 else \"0\"+str(i)\n",
    "    try :   \n",
    "        cols = sorted(train[f\"SNP_{target}\"].unique().tolist())  \n",
    "        train[f\"SNP_{target}\"] = train[f\"SNP_{target}\"].map(lambda x : 0 if x==cols[0] else (1 if x==cols[1] else 2))\n",
    "        test[f\"SNP_{target}\"] = test[f\"SNP_{target}\"].map(lambda x : 0 if x==cols[0] else (1 if x==cols[1] else 2))\n",
    "    except :\n",
    "        continue\n",
    "\n",
    "train.info(), test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed=0\n",
    "strategy1 = {0 : 40, 1 : 70, 2 : 50}\n",
    "\n",
    "under1 = RandomUnderSampler(sampling_strategy=strategy1, random_state=random_seed)\n",
    "under2 = EditedNearestNeighbours()\n",
    "under3 = RepeatedEditedNearestNeighbours()\n",
    "under4 = AllKNN()\n",
    "under5 = CondensedNearestNeighbour(random_state=random_seed)\n",
    "under6 = OneSidedSelection(random_state=random_seed)\n",
    "under7 = NeighbourhoodCleaningRule()\n",
    "\n",
    "X, y = train.iloc[:, 1:-1], train['class'].map(lambda x : 0 if x=='A' else (1 if x=='B' else 2)).values\n",
    "\n",
    "X1, y1 = under1.fit_resample(X, y)\n",
    "X2, y2 = under2.fit_resample(X, y)\n",
    "X3, y3 = under3.fit_resample(X, y)\n",
    "X4, y4 = under4.fit_resample(X, y)\n",
    "X5, y5 = under5.fit_resample(X, y)\n",
    "X6, y6 = under6.fit_resample(X, y)\n",
    "X7, y7 = under7.fit_resample(X, y)\n",
    "\n",
    "strategy2 = {0 : 100, 1 : 120, 2 : 110}\n",
    "\n",
    "over1 = SMOTEN(sampling_strategy=strategy2, random_state=random_seed)\n",
    "over2 = SMOTE(sampling_strategy=strategy2, random_state=random_seed)\n",
    "over3 = RandomOverSampler(sampling_strategy=strategy2, random_state=random_seed)\n",
    "\n",
    "X8, y8 = over1.fit_resample(X, y)\n",
    "X9, y9 = over2.fit_resample(X, y)\n",
    "X10, y10 = over3.fit_resample(X, y)\n",
    "\n",
    "df_train = pd.concat([X1, X2, X3, X4, X5, X6, X7, X8, X9, X10], ignore_index=True)\n",
    "df_train['class'] = list(y1)+list(y2)+list(y3)+list(y4)+list(y5)+list(y6)+list(y7)+list(y8)+list(y9)+list(y10)\n",
    "\n",
    "df = df_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed= 10\n",
    "strategy1 = {0 : 40, 1 : 70, 2 : 50}\n",
    "\n",
    "under1 = RandomUnderSampler(sampling_strategy=strategy1, random_state=random_seed)\n",
    "under2 = EditedNearestNeighbours()\n",
    "under3 = RepeatedEditedNearestNeighbours()\n",
    "under4 = AllKNN()\n",
    "under5 = CondensedNearestNeighbour(random_state=random_seed)\n",
    "under6 = OneSidedSelection(random_state=random_seed)\n",
    "under7 = NeighbourhoodCleaningRule()\n",
    "\n",
    "X, y = train.iloc[:, 1:-1], train['class'].map(lambda x : 0 if x=='A' else (1 if x=='B' else 2)).values\n",
    "\n",
    "X1, y1 = under1.fit_resample(X, y)\n",
    "X2, y2 = under2.fit_resample(X, y)\n",
    "X3, y3 = under3.fit_resample(X, y)\n",
    "X4, y4 = under4.fit_resample(X, y)\n",
    "X5, y5 = under5.fit_resample(X, y)\n",
    "X6, y6 = under6.fit_resample(X, y)\n",
    "X7, y7 = under7.fit_resample(X, y)\n",
    "\n",
    "strategy2 = {0 : 100, 1 : 120, 2 : 110}\n",
    "\n",
    "over1 = SMOTEN(sampling_strategy=strategy2, random_state=random_seed)\n",
    "over2 = SMOTE(sampling_strategy=strategy2, random_state=random_seed)\n",
    "over3 = RandomOverSampler(sampling_strategy=strategy2, random_state=random_seed)\n",
    "\n",
    "X8, y8 = over1.fit_resample(X, y)\n",
    "X9, y9 = over2.fit_resample(X, y)\n",
    "X10, y10 = over3.fit_resample(X, y)\n",
    "\n",
    "df_train = pd.concat([X1, X2, X3, X4, X5, X6, X7, X8, X9, X10], ignore_index=True)\n",
    "df_train['class'] = list(y1)+list(y2)+list(y3)+list(y4)+list(y5)+list(y6)+list(y7)+list(y8)+list(y9)+list(y10)\n",
    "\n",
    "df = pd.concat([df, df_train], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed= 100\n",
    "strategy1 = {0 : 40, 1 : 70, 2 : 50}\n",
    "\n",
    "under1 = RandomUnderSampler(sampling_strategy=strategy1, random_state=random_seed)\n",
    "under2 = EditedNearestNeighbours()\n",
    "under3 = RepeatedEditedNearestNeighbours()\n",
    "under4 = AllKNN()\n",
    "under5 = CondensedNearestNeighbour(random_state=random_seed)\n",
    "under6 = OneSidedSelection(random_state=random_seed)\n",
    "under7 = NeighbourhoodCleaningRule()\n",
    "\n",
    "X, y = train.iloc[:, 1:-1], train['class'].map(lambda x : 0 if x=='A' else (1 if x=='B' else 2)).values\n",
    "\n",
    "X1, y1 = under1.fit_resample(X, y)\n",
    "X2, y2 = under2.fit_resample(X, y)\n",
    "X3, y3 = under3.fit_resample(X, y)\n",
    "X4, y4 = under4.fit_resample(X, y)\n",
    "X5, y5 = under5.fit_resample(X, y)\n",
    "X6, y6 = under6.fit_resample(X, y)\n",
    "X7, y7 = under7.fit_resample(X, y)\n",
    "\n",
    "strategy2 = {0 : 100, 1 : 120, 2 : 110}\n",
    "\n",
    "over1 = SMOTEN(sampling_strategy=strategy2, random_state=random_seed)\n",
    "over2 = SMOTE(sampling_strategy=strategy2, random_state=random_seed)\n",
    "over3 = RandomOverSampler(sampling_strategy=strategy2, random_state=random_seed)\n",
    "\n",
    "X8, y8 = over1.fit_resample(X, y)\n",
    "X9, y9 = over2.fit_resample(X, y)\n",
    "X10, y10 = over3.fit_resample(X, y)\n",
    "\n",
    "df_train = pd.concat([X1, X2, X3, X4, X5, X6, X7, X8, X9, X10], ignore_index=True)\n",
    "df_train['class'] = list(y1)+list(y2)+list(y3)+list(y4)+list(y5)+list(y6)+list(y7)+list(y8)+list(y9)+list(y10)\n",
    "\n",
    "df = pd.concat([df, df_train], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trait</th>\n",
       "      <th>SNP_01</th>\n",
       "      <th>SNP_02</th>\n",
       "      <th>SNP_03</th>\n",
       "      <th>SNP_04</th>\n",
       "      <th>SNP_05</th>\n",
       "      <th>SNP_06</th>\n",
       "      <th>SNP_07</th>\n",
       "      <th>SNP_08</th>\n",
       "      <th>SNP_09</th>\n",
       "      <th>SNP_10</th>\n",
       "      <th>SNP_11</th>\n",
       "      <th>SNP_12</th>\n",
       "      <th>SNP_13</th>\n",
       "      <th>SNP_14</th>\n",
       "      <th>SNP_15</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>477 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    trait SNP_01 SNP_02 SNP_03 SNP_04 SNP_05 SNP_06 SNP_07 SNP_08 SNP_09  \\\n",
       "0       1      1      2      0      2      0      2      2      0      2   \n",
       "1       1      1      2      0      1      1      1      1      0      1   \n",
       "2       1      0      2      0      1      0      2      2      0      1   \n",
       "3       1      0      2      0      2      0      2      2      0      1   \n",
       "4       1      0      2      0      1      1      2      1      0      0   \n",
       "..    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "472     2      1      1      0      0      1      0      0      0      0   \n",
       "473     2      1      0      0      0      1      0      0      1      0   \n",
       "474     2      1      1      1      0      1      0      0      0      0   \n",
       "475     2      1      1      1      0      0      1      0      1      0   \n",
       "476     2      2      1      0      0      0      1      1      1      0   \n",
       "\n",
       "    SNP_10 SNP_11 SNP_12 SNP_13 SNP_14 SNP_15  class  \n",
       "0        0      2      1      2      1      1      0  \n",
       "1        0      2      1      2      1      0      0  \n",
       "2        1      1      1      2      1      2      0  \n",
       "3        1      2      1      2      1      2      0  \n",
       "4        0      2      2      2      2      2      0  \n",
       "..     ...    ...    ...    ...    ...    ...    ...  \n",
       "472      1      0      0      1      0      1      2  \n",
       "473      0      0      0      1      0      1      2  \n",
       "474      1      1      0      2      1      1      2  \n",
       "475      1      0      0      2      0      0      2  \n",
       "476      2      1      0      0      0      0      2  \n",
       "\n",
       "[477 rows x 17 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates(ignore_index=True, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([477, 3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
       "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
       "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
       "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
       "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
       "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
       "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
       "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
       "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
       "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
       "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
       "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
       "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
       "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
       "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
       "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
       "        252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
       "        266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279,\n",
       "        280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293,\n",
       "        294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307,\n",
       "        308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321,\n",
       "        322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335,\n",
       "        336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
       "        350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
       "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
       "        378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
       "        392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
       "        406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419,\n",
       "        420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
       "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
       "        448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461,\n",
       "        462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
       "        476])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(data.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 2,  ..., 2, 1, 1],\n",
       "        [1, 1, 2,  ..., 2, 1, 0],\n",
       "        [1, 0, 2,  ..., 2, 1, 2],\n",
       "        ...,\n",
       "        [2, 1, 1,  ..., 2, 1, 1],\n",
       "        [2, 1, 1,  ..., 2, 0, 0],\n",
       "        [2, 2, 1,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "shape mismatch: indexing tensors could not be broadcast together with shapes [477], [477, 16]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m one_hot_data[torch\u001b[39m.\u001b[39;49marange(data\u001b[39m.\u001b[39;49msize(\u001b[39m0\u001b[39;49m)), data] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mIndexError\u001b[0m: shape mismatch: indexing tensors could not be broadcast together with shapes [477], [477, 16]"
     ]
    }
   ],
   "source": [
    "one_hot_data[torch.arange(data.size(0)), data] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "shape mismatch: indexing tensors could not be broadcast together with shapes [477], [477, 16]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39m# Preprocess the data by one-hot encoding the categories\u001b[39;00m\n\u001b[1;32m      7\u001b[0m one_hot_data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(data\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), data\u001b[39m.\u001b[39mmax()\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m one_hot_data[torch\u001b[39m.\u001b[39;49marange(data\u001b[39m.\u001b[39;49msize(\u001b[39m0\u001b[39;49m)), data] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[39m# Define the model architecture\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mAutoencoder\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n",
      "\u001b[0;31mIndexError\u001b[0m: shape mismatch: indexing tensors could not be broadcast together with shapes [477], [477, 16]"
     ]
    }
   ],
   "source": [
    "X, y = df.iloc[:, :-1].to_numpy(), df['class']\n",
    "\n",
    "# Load the data\n",
    "data = torch.from_numpy(X)\n",
    "\n",
    "# Preprocess the data by one-hot encoding the categories\n",
    "one_hot_data = torch.zeros(data.size(0), data.max()+1)\n",
    "one_hot_data[torch.arange(data.size(0)), data] = 1\n",
    "\n",
    "# Define the model architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.encoding_dim = encoding_dim\n",
    "\n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(input_dim, 32)\n",
    "        self.fc2 = nn.Linear(32, encoding_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.fc3 = nn.Linear(encoding_dim, 32)\n",
    "        self.fc4 = nn.Linear(32, input_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = torch.relu(self.fc1(x))\n",
    "        return self.fc2(h1)\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = torch.relu(self.fc3(z))\n",
    "        return self.fc4(h3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        return self.decode(z)\n",
    "\n",
    "# Instantiate the model\n",
    "model = Autoencoder(input_dim=one_hot_data.size(1), encoding_dim=16)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i, x in enumerate(one_hot_data):\n",
    "        optimizer.zero_grad()\n",
    "        recon_x = model(x)\n",
    "        loss = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "# Use the encoder to generate the encoded representation of the data\n",
    "encoder = nn.Sequential(*list(model.children())[:2])\n",
    "encoded_data = encoder(one_hot_data)\n",
    "\n",
    "# Use the decoder to generate new variables from the encoded representation\n",
    "decoder = nn.Sequential(*list(model.children())[2:])\n",
    "new_variables = decoder(encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "shape mismatch: indexing tensors could not be broadcast together with shapes [477], [477, 16]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39m# Preprocess the data by one-hot encoding the categories\u001b[39;00m\n\u001b[1;32m      9\u001b[0m one_hot_data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(data\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), data\u001b[39m.\u001b[39mmax()\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m one_hot_data[torch\u001b[39m.\u001b[39;49marange(data\u001b[39m.\u001b[39;49msize(\u001b[39m0\u001b[39;49m)), data] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[39m# Define the generator network\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mGenerator\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n",
      "\u001b[0;31mIndexError\u001b[0m: shape mismatch: indexing tensors could not be broadcast together with shapes [477], [477, 16]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load the data\n",
    "data = torch.from_numpy(X)\n",
    "\n",
    "# Preprocess the data by one-hot encoding the categories\n",
    "one_hot_data = torch.zeros(data.size(0), data.max()+1)\n",
    "one_hot_data[torch.arange(data.size(0)), data] = 1\n",
    "\n",
    "# Define the generator network\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, 32)\n",
    "        self.fc2 = nn.Linear(32, output_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        h1 = torch.relu(self.fc1(z))\n",
    "        return self.fc2(h1)\n",
    "\n",
    "# Define the discriminator network\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, 32)\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1 = torch.relu(self.fc1(x))\n",
    "        return self.fc2(h1)\n",
    "\n",
    "# Instantiate the generator and discriminator networks\n",
    "generator = Generator(input_dim=100, output_dim=one_hot_data.size(1))\n",
    "discriminator = Discriminator(input_dim=one_hot_data.size(1))\n",
    "\n",
    "# Define the loss function and optimizers\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=1e-3)\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=1e-3)\n",
    "\n",
    "# Train the GAN\n",
    "for epoch in range(10):\n",
    "    for i, real_data in enumerate(one_hot_data):\n",
    "        # Generate fake data\n",
    "        noise = torch.randn(100)\n",
    "        fake_data = generator(noise)\n",
    "\n",
    "        # Train the discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        loss_real = F.binary_cross_entropy(discriminator(real_data), torch.ones_like(discriminator(real_data)))\n",
    "        loss_fake = F.binary_cross_entropy(discriminator(fake_data), torch.zeros_like(discriminator(fake_data)))\n",
    "        loss_D = (loss_real + loss_fake) / 2\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Train the generator\n",
    "        optimizer_G.zero_grad()\n",
    "        loss_G = F.binary_cross_entropy(discriminator(fake_data), torch.ones_like(discriminator(fake_data)))\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "# Use the generator to generate new variables\n",
    "new_variables = generator(torch.randn(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Load the data\n",
    "data = np.load('categorical_data.npy')\n",
    "\n",
    "# Preprocess the data by one-hot encoding the categories\n",
    "one_hot_data = np.zeros((data.size, data.max()+1))\n",
    "one_hot_data[np.arange(data.size), data] = 1\n",
    "\n",
    "# Use t-SNE to reduce the dimensionality of the data\n",
    "tsne = TSNE(n_components=2)\n",
    "new_variables = tsne.fit_transform(one_hot_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
