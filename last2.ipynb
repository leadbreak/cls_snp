{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, sampler\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC, SMOTEN, RandomOverSampler, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler, EditedNearestNeighbours, RepeatedEditedNearestNeighbours, AllKNN, CondensedNearestNeighbour, OneSidedSelection, NeighbourhoodCleaningRule\n",
    "\n",
    "import shap\n",
    "import catboost\n",
    "from catboost import Pool, cv\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 248 entries, 0 to 247\n",
      "Data columns (total 18 columns):\n",
      " #   Column  Non-Null Count  Dtype   \n",
      "---  ------  --------------  -----   \n",
      " 0   id      248 non-null    object  \n",
      " 1   trait   248 non-null    category\n",
      " 2   SNP_01  248 non-null    category\n",
      " 3   SNP_02  248 non-null    category\n",
      " 4   SNP_03  248 non-null    category\n",
      " 5   SNP_04  248 non-null    category\n",
      " 6   SNP_05  248 non-null    category\n",
      " 7   SNP_06  248 non-null    category\n",
      " 8   SNP_07  248 non-null    category\n",
      " 9   SNP_08  248 non-null    category\n",
      " 10  SNP_09  248 non-null    category\n",
      " 11  SNP_10  248 non-null    category\n",
      " 12  SNP_11  248 non-null    category\n",
      " 13  SNP_12  248 non-null    category\n",
      " 14  SNP_13  248 non-null    category\n",
      " 15  SNP_14  248 non-null    category\n",
      " 16  SNP_15  248 non-null    category\n",
      " 17  class   248 non-null    object  \n",
      "dtypes: category(16), object(2)\n",
      "memory usage: 9.9+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 175 entries, 0 to 174\n",
      "Data columns (total 17 columns):\n",
      " #   Column  Non-Null Count  Dtype   \n",
      "---  ------  --------------  -----   \n",
      " 0   id      175 non-null    object  \n",
      " 1   trait   175 non-null    category\n",
      " 2   SNP_01  175 non-null    category\n",
      " 3   SNP_02  175 non-null    category\n",
      " 4   SNP_03  175 non-null    category\n",
      " 5   SNP_04  175 non-null    category\n",
      " 6   SNP_05  175 non-null    category\n",
      " 7   SNP_06  175 non-null    category\n",
      " 8   SNP_07  175 non-null    category\n",
      " 9   SNP_08  175 non-null    category\n",
      " 10  SNP_09  175 non-null    category\n",
      " 11  SNP_10  175 non-null    category\n",
      " 12  SNP_11  175 non-null    category\n",
      " 13  SNP_12  175 non-null    category\n",
      " 14  SNP_13  175 non-null    category\n",
      " 15  SNP_14  175 non-null    category\n",
      " 16  SNP_15  175 non-null    category\n",
      "dtypes: category(16), object(1)\n",
      "memory usage: 6.3+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"./data/train.csv\").drop(columns=['father', 'mother', 'gender'])\n",
    "train.drop_duplicates(subset=train.columns.tolist()[5:20], inplace=True, ignore_index=True)\n",
    "test = pd.read_csv(\"./data/test.csv\").drop(columns=['father', 'mother', 'gender'])\n",
    "\n",
    "train.iloc[:, 1:-1] = train.iloc[:, 1:-1].astype('category')\n",
    "test.iloc[:, 1:] = test.iloc[:, 1:].astype('category')\n",
    "\n",
    "answer = np.zeros(len(test)) - 1\n",
    "\n",
    "train.info(), test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 1862.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 248 entries, 0 to 247\n",
      "Data columns (total 18 columns):\n",
      " #   Column  Non-Null Count  Dtype   \n",
      "---  ------  --------------  -----   \n",
      " 0   id      248 non-null    object  \n",
      " 1   trait   248 non-null    category\n",
      " 2   SNP_01  248 non-null    category\n",
      " 3   SNP_02  248 non-null    category\n",
      " 4   SNP_03  248 non-null    category\n",
      " 5   SNP_04  248 non-null    category\n",
      " 6   SNP_05  248 non-null    category\n",
      " 7   SNP_06  248 non-null    category\n",
      " 8   SNP_07  248 non-null    category\n",
      " 9   SNP_08  248 non-null    category\n",
      " 10  SNP_09  248 non-null    category\n",
      " 11  SNP_10  248 non-null    category\n",
      " 12  SNP_11  248 non-null    category\n",
      " 13  SNP_12  248 non-null    category\n",
      " 14  SNP_13  248 non-null    category\n",
      " 15  SNP_14  248 non-null    category\n",
      " 16  SNP_15  248 non-null    category\n",
      " 17  class   248 non-null    object  \n",
      "dtypes: category(16), object(2)\n",
      "memory usage: 9.9+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 175 entries, 0 to 174\n",
      "Data columns (total 17 columns):\n",
      " #   Column  Non-Null Count  Dtype   \n",
      "---  ------  --------------  -----   \n",
      " 0   id      175 non-null    object  \n",
      " 1   trait   175 non-null    category\n",
      " 2   SNP_01  175 non-null    category\n",
      " 3   SNP_02  175 non-null    category\n",
      " 4   SNP_03  175 non-null    category\n",
      " 5   SNP_04  175 non-null    category\n",
      " 6   SNP_05  175 non-null    category\n",
      " 7   SNP_06  175 non-null    category\n",
      " 8   SNP_07  175 non-null    category\n",
      " 9   SNP_08  175 non-null    category\n",
      " 10  SNP_09  175 non-null    category\n",
      " 11  SNP_10  175 non-null    category\n",
      " 12  SNP_11  175 non-null    category\n",
      " 13  SNP_12  175 non-null    category\n",
      " 14  SNP_13  175 non-null    category\n",
      " 15  SNP_14  175 non-null    category\n",
      " 16  SNP_15  175 non-null    category\n",
      "dtypes: category(16), object(1)\n",
      "memory usage: 6.3+ KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text 형태의 categorical 변수들을 숫자형태로 변경\n",
    "\n",
    "for i in tqdm(range(1, 15+1)) :\n",
    "    target = str(i) if i >= 10 else \"0\"+str(i)\n",
    "    try :   \n",
    "        cols = sorted(train[f\"SNP_{target}\"].unique().tolist())  \n",
    "        train[f\"SNP_{target}\"] = train[f\"SNP_{target}\"].map(lambda x : 0 if x==cols[0] else (1 if x==cols[1] else 2))\n",
    "        test[f\"SNP_{target}\"] = test[f\"SNP_{target}\"].map(lambda x : 0 if x==cols[0] else (1 if x==cols[1] else 2))\n",
    "    except :\n",
    "        continue\n",
    "\n",
    "train.info(), test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([248, 47]), torch.Size([175, 47]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = train.iloc[:, 1:-1].to_numpy(), train['class'].map(lambda x : 0 if x=='A' else(1 if x=='B' else 2)).values\n",
    "X_test = test.iloc[:,1:].to_numpy()\n",
    "\n",
    "# Load the data\n",
    "train_data = torch.from_numpy(X)\n",
    "test_data = torch.from_numpy(X_test)\n",
    "\n",
    "# Preprocess the data by one-hot encoding the categories\n",
    "one_hot_data_01 = F.one_hot(train_data[:,:1]-1, num_classes=2).view(len(X), 2).float()\n",
    "one_hot_data_02 = F.one_hot(train_data[:,1:], num_classes=3).view(len(X), 3*train_data[:,1:].size(1)).float()\n",
    "\n",
    "one_hot_train = torch.concat([one_hot_data_01, one_hot_data_02], axis=1)\n",
    "\n",
    "one_hot_data_01 = F.one_hot(test_data[:,:1]-1, num_classes=2).view(len(X_test), 2).float()\n",
    "one_hot_data_02 = F.one_hot(test_data[:,1:], num_classes=3).view(len(X_test), 3*test_data[:,1:].size(1)).float()\n",
    "\n",
    "one_hot_test = torch.concat([one_hot_data_01, one_hot_data_02], axis=1)\n",
    "\n",
    "one_hot_train.shape, one_hot_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_heads, num_classes):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.attention = nn.MultiheadAttention(hidden_dim, num_heads)\n",
    "        self.output_layer = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.attention(x, x, x)[0]\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, num_epochs, batch_size, learning_rate):\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    step_size = 20\n",
    "    gamma=1\n",
    "\n",
    "    # define the scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    train_data, val_data = data, data\n",
    "\n",
    "    # Create DataLoaders for the training and validation sets\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Loop over the number of epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # Set the model to training mode\n",
    "        model.train()\n",
    "\n",
    "        # Loop over the training data\n",
    "        for x, y in train_loader:\n",
    "            # Move the data to the correct device\n",
    "            x, y = x.to(device), torch.Tensor(y).to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            # Backward pass and optimization step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # update the learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # Initialize the validation loss and accuracy\n",
    "        val_loss = 0.0\n",
    "        val_acc = 0.0\n",
    "\n",
    "        # Loop over the validation data\n",
    "        for x, y in val_loader:\n",
    "            # Move the data to the correct device\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            # Update the validation loss and accuracy\n",
    "            val_loss += loss.item()\n",
    "            val_acc += (logits.argmax(1) == y).float().mean().item()\n",
    "\n",
    "        # Calculate the average validation loss and accuracy\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc /= len(val_loader)\n",
    "\n",
    "        # Print the epoch, loss, and accuracy\n",
    "        print(f\"Epoch {epoch+1}: loss = {loss:.4f}, val_loss = {val_loss:.4f}, val_accuracy = {val_acc:.4f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss = 1.0543, val_loss = 1.0676, val_accuracy = 0.4492\n",
      "Epoch 2: loss = 1.0411, val_loss = 1.0659, val_accuracy = 0.4492\n",
      "Epoch 3: loss = 1.0414, val_loss = 1.0634, val_accuracy = 0.4492\n",
      "Epoch 4: loss = 1.0422, val_loss = 1.0580, val_accuracy = 0.4492\n",
      "Epoch 5: loss = 1.0378, val_loss = 1.0494, val_accuracy = 0.4492\n",
      "Epoch 6: loss = 1.0126, val_loss = 1.0235, val_accuracy = 0.4492\n",
      "Epoch 7: loss = 0.9324, val_loss = 0.9463, val_accuracy = 0.4967\n",
      "Epoch 8: loss = 0.7326, val_loss = 0.7602, val_accuracy = 0.6791\n",
      "Epoch 9: loss = 0.5124, val_loss = 0.5557, val_accuracy = 0.6914\n",
      "Epoch 10: loss = 0.4235, val_loss = 0.4728, val_accuracy = 0.7031\n",
      "Epoch 11: loss = 0.3786, val_loss = 0.4040, val_accuracy = 0.8644\n",
      "Epoch 12: loss = 0.2910, val_loss = 0.3209, val_accuracy = 0.9124\n",
      "Epoch 13: loss = 0.1850, val_loss = 0.2279, val_accuracy = 0.9280\n",
      "Epoch 14: loss = 0.1008, val_loss = 0.1496, val_accuracy = 0.9526\n",
      "Epoch 15: loss = 0.0537, val_loss = 0.1097, val_accuracy = 0.9565\n",
      "Epoch 16: loss = 0.0224, val_loss = 0.0901, val_accuracy = 0.9609\n",
      "Epoch 17: loss = 0.0100, val_loss = 0.0779, val_accuracy = 0.9727\n",
      "Epoch 18: loss = 0.0063, val_loss = 0.0669, val_accuracy = 0.9766\n",
      "Epoch 19: loss = 0.0074, val_loss = 0.0575, val_accuracy = 0.9805\n",
      "Epoch 20: loss = 0.0087, val_loss = 0.0487, val_accuracy = 0.9805\n",
      "Epoch 21: loss = 0.0105, val_loss = 0.0493, val_accuracy = 0.9844\n",
      "Epoch 22: loss = 0.0372, val_loss = 0.0628, val_accuracy = 0.9643\n",
      "Epoch 23: loss = 0.0132, val_loss = 0.0364, val_accuracy = 0.9883\n",
      "Epoch 24: loss = 0.0122, val_loss = 0.0448, val_accuracy = 0.9922\n",
      "Epoch 25: loss = 0.0156, val_loss = 0.0350, val_accuracy = 0.9844\n",
      "Epoch 26: loss = 0.0110, val_loss = 0.0304, val_accuracy = 0.9844\n",
      "Epoch 27: loss = 0.0086, val_loss = 0.0397, val_accuracy = 0.9922\n",
      "Epoch 28: loss = 0.0067, val_loss = 0.0256, val_accuracy = 0.9922\n",
      "Epoch 29: loss = 0.0062, val_loss = 0.0240, val_accuracy = 0.9922\n",
      "Epoch 30: loss = 0.0048, val_loss = 0.0217, val_accuracy = 0.9922\n",
      "Epoch 31: loss = 0.0071, val_loss = 0.0213, val_accuracy = 0.9922\n",
      "Epoch 32: loss = 0.0046, val_loss = 0.0175, val_accuracy = 0.9961\n",
      "Epoch 33: loss = 0.0052, val_loss = 0.0156, val_accuracy = 0.9961\n",
      "Epoch 34: loss = 0.0072, val_loss = 0.0149, val_accuracy = 0.9961\n",
      "Epoch 35: loss = 0.0056, val_loss = 0.0132, val_accuracy = 0.9961\n",
      "Epoch 36: loss = 0.0054, val_loss = 0.0133, val_accuracy = 0.9961\n",
      "Epoch 37: loss = 0.0035, val_loss = 0.0112, val_accuracy = 0.9961\n",
      "Epoch 38: loss = 0.0026, val_loss = 0.0244, val_accuracy = 0.9922\n",
      "Epoch 39: loss = 0.0156, val_loss = 0.0235, val_accuracy = 0.9877\n",
      "Epoch 40: loss = 0.0047, val_loss = 0.0111, val_accuracy = 0.9961\n",
      "Epoch 41: loss = 0.0020, val_loss = 0.0146, val_accuracy = 0.9961\n",
      "Epoch 42: loss = 0.0085, val_loss = 0.0113, val_accuracy = 1.0000\n",
      "Epoch 43: loss = 0.0047, val_loss = 0.0090, val_accuracy = 1.0000\n",
      "Epoch 44: loss = 0.0022, val_loss = 0.0180, val_accuracy = 0.9922\n",
      "Epoch 45: loss = 0.0049, val_loss = 0.0090, val_accuracy = 1.0000\n",
      "Epoch 46: loss = 0.0015, val_loss = 0.0120, val_accuracy = 0.9961\n",
      "Epoch 47: loss = 0.0025, val_loss = 0.0106, val_accuracy = 0.9961\n",
      "Epoch 48: loss = 0.0027, val_loss = 0.0085, val_accuracy = 0.9961\n",
      "Epoch 49: loss = 0.0046, val_loss = 0.0064, val_accuracy = 1.0000\n",
      "Epoch 50: loss = 0.0029, val_loss = 0.0060, val_accuracy = 0.9961\n",
      "Epoch 51: loss = 0.0043, val_loss = 0.0048, val_accuracy = 1.0000\n",
      "Epoch 52: loss = 0.0027, val_loss = 0.0052, val_accuracy = 1.0000\n",
      "Epoch 53: loss = 0.0115, val_loss = 0.0133, val_accuracy = 0.9961\n",
      "Epoch 54: loss = 0.0008, val_loss = 0.0143, val_accuracy = 0.9961\n",
      "Epoch 55: loss = 0.0009, val_loss = 0.0179, val_accuracy = 0.9961\n",
      "Epoch 56: loss = 0.0015, val_loss = 0.0077, val_accuracy = 1.0000\n",
      "Epoch 57: loss = 0.0014, val_loss = 0.0058, val_accuracy = 1.0000\n",
      "Epoch 58: loss = 0.0011, val_loss = 0.0117, val_accuracy = 0.9961\n",
      "Epoch 59: loss = 0.0016, val_loss = 0.0044, val_accuracy = 1.0000\n",
      "Epoch 60: loss = 0.0013, val_loss = 0.0034, val_accuracy = 1.0000\n",
      "Epoch 61: loss = 0.0008, val_loss = 0.0028, val_accuracy = 1.0000\n",
      "Epoch 62: loss = 0.0010, val_loss = 0.0022, val_accuracy = 1.0000\n",
      "Epoch 63: loss = 0.0029, val_loss = 0.0040, val_accuracy = 1.0000\n",
      "Epoch 64: loss = 0.0006, val_loss = 0.0026, val_accuracy = 1.0000\n",
      "Epoch 65: loss = 0.0005, val_loss = 0.0044, val_accuracy = 1.0000\n",
      "Epoch 66: loss = 0.0033, val_loss = 0.0031, val_accuracy = 1.0000\n",
      "Epoch 67: loss = 0.0005, val_loss = 0.0017, val_accuracy = 1.0000\n",
      "Epoch 68: loss = 0.0002, val_loss = 0.0028, val_accuracy = 1.0000\n",
      "Epoch 69: loss = 0.0005, val_loss = 0.0026, val_accuracy = 1.0000\n",
      "Epoch 70: loss = 0.0002, val_loss = 0.0013, val_accuracy = 1.0000\n",
      "Epoch 71: loss = 0.0001, val_loss = 0.0026, val_accuracy = 1.0000\n",
      "Epoch 72: loss = 0.0003, val_loss = 0.0020, val_accuracy = 1.0000\n",
      "Epoch 73: loss = 0.0001, val_loss = 0.0009, val_accuracy = 1.0000\n",
      "Epoch 74: loss = 0.0004, val_loss = 0.0014, val_accuracy = 1.0000\n",
      "Epoch 75: loss = 0.0010, val_loss = 0.0028, val_accuracy = 1.0000\n",
      "Epoch 76: loss = 0.0003, val_loss = 0.0018, val_accuracy = 1.0000\n",
      "Epoch 77: loss = 0.0001, val_loss = 0.0016, val_accuracy = 1.0000\n",
      "Epoch 78: loss = 0.0013, val_loss = 0.0032, val_accuracy = 1.0000\n",
      "Epoch 79: loss = 0.0010, val_loss = 0.0025, val_accuracy = 1.0000\n",
      "Epoch 80: loss = 0.0001, val_loss = 0.0005, val_accuracy = 1.0000\n",
      "Epoch 81: loss = 0.0001, val_loss = 0.0012, val_accuracy = 1.0000\n",
      "Epoch 82: loss = 0.0001, val_loss = 0.0004, val_accuracy = 1.0000\n",
      "Epoch 83: loss = 0.0002, val_loss = 0.0005, val_accuracy = 1.0000\n",
      "Epoch 84: loss = 0.0003, val_loss = 0.0006, val_accuracy = 1.0000\n",
      "Epoch 85: loss = 0.0002, val_loss = 0.0005, val_accuracy = 1.0000\n",
      "Epoch 86: loss = 0.0001, val_loss = 0.0004, val_accuracy = 1.0000\n",
      "Epoch 87: loss = 0.0002, val_loss = 0.0004, val_accuracy = 1.0000\n",
      "Epoch 88: loss = 0.0001, val_loss = 0.0003, val_accuracy = 1.0000\n",
      "Epoch 89: loss = 0.0008, val_loss = 0.0011, val_accuracy = 1.0000\n",
      "Epoch 90: loss = 0.0001, val_loss = 0.0002, val_accuracy = 1.0000\n",
      "Epoch 91: loss = 0.0001, val_loss = 0.0002, val_accuracy = 1.0000\n",
      "Epoch 92: loss = 0.0007, val_loss = 0.0005, val_accuracy = 1.0000\n",
      "Epoch 93: loss = 0.0001, val_loss = 0.0002, val_accuracy = 1.0000\n",
      "Epoch 94: loss = 0.0000, val_loss = 0.0007, val_accuracy = 1.0000\n",
      "Epoch 95: loss = 0.0000, val_loss = 0.0009, val_accuracy = 1.0000\n",
      "Epoch 96: loss = 0.0001, val_loss = 0.0070, val_accuracy = 0.9961\n",
      "Epoch 97: loss = 0.0001, val_loss = 0.0102, val_accuracy = 0.9922\n",
      "Epoch 98: loss = 0.0000, val_loss = 0.0262, val_accuracy = 0.9922\n",
      "Epoch 99: loss = 0.0001, val_loss = 0.0035, val_accuracy = 1.0000\n",
      "Epoch 100: loss = 0.0016, val_loss = 0.0040, val_accuracy = 1.0000\n",
      "Epoch 101: loss = 0.0019, val_loss = 0.0247, val_accuracy = 0.9961\n",
      "Epoch 102: loss = 0.0009, val_loss = 0.0072, val_accuracy = 0.9961\n",
      "Epoch 103: loss = 0.0051, val_loss = 0.0672, val_accuracy = 0.9766\n",
      "Epoch 104: loss = 0.0199, val_loss = 0.0253, val_accuracy = 0.9916\n",
      "Epoch 105: loss = 0.0042, val_loss = 0.0238, val_accuracy = 0.9961\n",
      "Epoch 106: loss = 0.0008, val_loss = 0.0156, val_accuracy = 0.9883\n",
      "Epoch 107: loss = 0.0021, val_loss = 0.0198, val_accuracy = 0.9922\n",
      "Epoch 108: loss = 0.0010, val_loss = 0.0106, val_accuracy = 0.9961\n",
      "Epoch 109: loss = 0.0011, val_loss = 0.0123, val_accuracy = 0.9922\n",
      "Epoch 110: loss = 0.0017, val_loss = 0.0070, val_accuracy = 1.0000\n",
      "Epoch 111: loss = 0.0020, val_loss = 0.0067, val_accuracy = 1.0000\n",
      "Epoch 112: loss = 0.0013, val_loss = 0.0053, val_accuracy = 1.0000\n",
      "Epoch 113: loss = 0.0011, val_loss = 0.0047, val_accuracy = 1.0000\n",
      "Epoch 114: loss = 0.0011, val_loss = 0.0042, val_accuracy = 1.0000\n",
      "Epoch 115: loss = 0.0019, val_loss = 0.0051, val_accuracy = 1.0000\n",
      "Epoch 116: loss = 0.0017, val_loss = 0.0043, val_accuracy = 1.0000\n",
      "Epoch 117: loss = 0.0010, val_loss = 0.0031, val_accuracy = 1.0000\n",
      "Epoch 118: loss = 0.0006, val_loss = 0.0033, val_accuracy = 1.0000\n",
      "Epoch 119: loss = 0.0008, val_loss = 0.0024, val_accuracy = 1.0000\n",
      "Epoch 120: loss = 0.0009, val_loss = 0.0023, val_accuracy = 1.0000\n",
      "Epoch 121: loss = 0.0006, val_loss = 0.0018, val_accuracy = 1.0000\n",
      "Epoch 122: loss = 0.0006, val_loss = 0.0016, val_accuracy = 1.0000\n",
      "Epoch 123: loss = 0.0004, val_loss = 0.0015, val_accuracy = 1.0000\n",
      "Epoch 124: loss = 0.0004, val_loss = 0.0013, val_accuracy = 1.0000\n",
      "Epoch 125: loss = 0.0006, val_loss = 0.0016, val_accuracy = 1.0000\n",
      "Epoch 126: loss = 0.0005, val_loss = 0.0014, val_accuracy = 1.0000\n",
      "Epoch 127: loss = 0.0003, val_loss = 0.0010, val_accuracy = 1.0000\n",
      "Epoch 128: loss = 0.0002, val_loss = 0.0011, val_accuracy = 1.0000\n",
      "Epoch 129: loss = 0.0003, val_loss = 0.0008, val_accuracy = 1.0000\n",
      "Epoch 130: loss = 0.0003, val_loss = 0.0008, val_accuracy = 1.0000\n",
      "Epoch 131: loss = 0.0006, val_loss = 0.0010, val_accuracy = 1.0000\n",
      "Epoch 132: loss = 0.0006, val_loss = 0.0010, val_accuracy = 1.0000\n",
      "Epoch 133: loss = 0.0004, val_loss = 0.0007, val_accuracy = 1.0000\n",
      "Epoch 134: loss = 0.0003, val_loss = 0.0006, val_accuracy = 1.0000\n",
      "Epoch 135: loss = 0.0002, val_loss = 0.0005, val_accuracy = 1.0000\n",
      "Epoch 136: loss = 0.0001, val_loss = 0.0006, val_accuracy = 1.0000\n",
      "Epoch 137: loss = 0.0001, val_loss = 0.0006, val_accuracy = 1.0000\n",
      "Epoch 138: loss = 0.0002, val_loss = 0.0005, val_accuracy = 1.0000\n",
      "Epoch 139: loss = 0.0010, val_loss = 0.0019, val_accuracy = 1.0000\n",
      "Epoch 140: loss = 0.0005, val_loss = 0.0011, val_accuracy = 1.0000\n",
      "Epoch 141: loss = 0.0001, val_loss = 0.0004, val_accuracy = 1.0000\n",
      "Epoch 142: loss = 0.0001, val_loss = 0.0008, val_accuracy = 1.0000\n",
      "Epoch 143: loss = 0.0001, val_loss = 0.0013, val_accuracy = 1.0000\n",
      "Epoch 144: loss = 0.0001, val_loss = 0.0004, val_accuracy = 1.0000\n",
      "Epoch 145: loss = 0.0002, val_loss = 0.0005, val_accuracy = 1.0000\n",
      "Epoch 146: loss = 0.0003, val_loss = 0.0006, val_accuracy = 1.0000\n",
      "Epoch 147: loss = 0.0003, val_loss = 0.0005, val_accuracy = 1.0000\n",
      "Epoch 148: loss = 0.0001, val_loss = 0.0003, val_accuracy = 1.0000\n",
      "Epoch 149: loss = 0.0000, val_loss = 0.0002, val_accuracy = 1.0000\n",
      "Epoch 150: loss = 0.0001, val_loss = 0.0003, val_accuracy = 1.0000\n",
      "Epoch 151: loss = 0.0001, val_loss = 0.0003, val_accuracy = 1.0000\n",
      "Epoch 152: loss = 0.0001, val_loss = 0.0002, val_accuracy = 1.0000\n",
      "Epoch 153: loss = 0.0000, val_loss = 0.0002, val_accuracy = 1.0000\n",
      "Epoch 154: loss = 0.0000, val_loss = 0.0003, val_accuracy = 1.0000\n",
      "Epoch 155: loss = 0.0000, val_loss = 0.0003, val_accuracy = 1.0000\n",
      "Epoch 156: loss = 0.0000, val_loss = 0.0002, val_accuracy = 1.0000\n",
      "Epoch 157: loss = 0.0001, val_loss = 0.0002, val_accuracy = 1.0000\n",
      "Epoch 158: loss = 0.0001, val_loss = 0.0002, val_accuracy = 1.0000\n",
      "Epoch 159: loss = 0.0001, val_loss = 0.0002, val_accuracy = 1.0000\n",
      "Epoch 160: loss = 0.0001, val_loss = 0.0002, val_accuracy = 1.0000\n",
      "Epoch 161: loss = 0.0001, val_loss = 0.0001, val_accuracy = 1.0000\n",
      "Epoch 162: loss = 0.0001, val_loss = 0.0001, val_accuracy = 1.0000\n",
      "Epoch 163: loss = 0.0001, val_loss = 0.0001, val_accuracy = 1.0000\n",
      "Epoch 164: loss = 0.0001, val_loss = 0.0001, val_accuracy = 1.0000\n",
      "Epoch 165: loss = 0.0001, val_loss = 0.0001, val_accuracy = 1.0000\n",
      "Epoch 166: loss = 0.0001, val_loss = 0.0002, val_accuracy = 1.0000\n",
      "Epoch 167: loss = 0.0001, val_loss = 0.0002, val_accuracy = 1.0000\n",
      "Epoch 168: loss = 0.0000, val_loss = 0.0001, val_accuracy = 1.0000\n",
      "Epoch 169: loss = 0.0000, val_loss = 0.0003, val_accuracy = 1.0000\n",
      "Epoch 170: loss = 0.0000, val_loss = 0.0002, val_accuracy = 1.0000\n",
      "Epoch 171: loss = 0.0000, val_loss = 0.0001, val_accuracy = 1.0000\n",
      "Epoch 172: loss = 0.0000, val_loss = 0.0001, val_accuracy = 1.0000\n",
      "Epoch 173: loss = 0.0000, val_loss = 0.0001, val_accuracy = 1.0000\n",
      "Epoch 174: loss = 0.0001, val_loss = 0.0002, val_accuracy = 1.0000\n",
      "Epoch 175: loss = 0.0000, val_loss = 0.0001, val_accuracy = 1.0000\n",
      "Epoch 176: loss = 0.0000, val_loss = 0.0001, val_accuracy = 1.0000\n",
      "Epoch 177: loss = 0.0000, val_loss = 0.0001, val_accuracy = 1.0000\n",
      "Epoch 178: loss = 0.0000, val_loss = 0.0001, val_accuracy = 1.0000\n",
      "Epoch 179: loss = 0.0000, val_loss = 0.0001, val_accuracy = 1.0000\n",
      "Epoch 180: loss = 0.0000, val_loss = 0.0001, val_accuracy = 1.0000\n",
      "Epoch 181: loss = 0.0000, val_loss = 0.0001, val_accuracy = 1.0000\n",
      "Epoch 182: loss = 0.0000, val_loss = 0.0001, val_accuracy = 1.0000\n",
      "Epoch 183: loss = 0.0000, val_loss = 0.0001, val_accuracy = 1.0000\n",
      "Epoch 184: loss = 0.0000, val_loss = 0.0001, val_accuracy = 1.0000\n",
      "Epoch 185: loss = 0.0000, val_loss = 0.0001, val_accuracy = 1.0000\n",
      "Epoch 186: loss = 0.0000, val_loss = 0.0001, val_accuracy = 1.0000\n",
      "Epoch 187: loss = 0.0000, val_loss = 0.0001, val_accuracy = 1.0000\n",
      "Epoch 188: loss = 0.0000, val_loss = 0.0001, val_accuracy = 1.0000\n",
      "Epoch 189: loss = 0.0000, val_loss = 0.0001, val_accuracy = 1.0000\n",
      "Epoch 190: loss = 0.0000, val_loss = 0.0001, val_accuracy = 1.0000\n",
      "Epoch 191: loss = 0.0000, val_loss = 0.0001, val_accuracy = 1.0000\n",
      "Epoch 192: loss = 0.0000, val_loss = 0.0001, val_accuracy = 1.0000\n",
      "Epoch 193: loss = 0.0000, val_loss = 0.0001, val_accuracy = 1.0000\n",
      "Epoch 194: loss = 0.0000, val_loss = 0.0000, val_accuracy = 1.0000\n",
      "Epoch 195: loss = 0.0000, val_loss = 0.0001, val_accuracy = 1.0000\n",
      "Epoch 196: loss = 0.0000, val_loss = 0.0001, val_accuracy = 1.0000\n",
      "Epoch 197: loss = 0.0000, val_loss = 0.0000, val_accuracy = 1.0000\n",
      "Epoch 198: loss = 0.0000, val_loss = 0.0000, val_accuracy = 1.0000\n",
      "Epoch 199: loss = 0.0000, val_loss = 0.0000, val_accuracy = 1.0000\n",
      "Epoch 200: loss = 0.0000, val_loss = 0.0001, val_accuracy = 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Transformer(input_dim=47, hidden_dim=64, num_heads=8, num_classes=3)\n",
    "\n",
    "# Create the dataset and data loader\n",
    "dataset = SparseDataset(one_hot_train, y, transform=None)\n",
    "\n",
    "# Train the model\n",
    "model = train(model, dataset, num_epochs=200, batch_size=64, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 2, 0, 1, 2, 1, 0, 0, 2, 1, 2, 0, 1, 1, 0, 1, 1, 2, 1, 1,\n",
       "       1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 2, 0, 1, 2, 1, 1, 2, 0, 1, 2, 1,\n",
       "       1, 1, 1, 2, 1, 2, 0, 1, 0, 1, 1, 1, 2, 0, 1, 2, 0, 1, 2, 2, 2, 0,\n",
       "       1, 0, 0, 1, 1, 1, 0, 0, 2, 1, 2, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 1,\n",
       "       2, 0, 1, 1, 2, 1, 1, 2, 0, 1, 0, 2, 0, 1, 1, 2, 0, 0, 2, 1, 0, 1,\n",
       "       2, 1, 1, 1, 1, 0, 0, 2, 1, 2, 0, 1, 1, 2, 2, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 2, 0, 0, 1, 0, 0, 0, 2, 1, 1, 1, 0, 1, 2, 0, 0, 1, 0, 1,\n",
       "       1, 0, 0, 1, 2, 0, 1, 2, 1, 1, 0, 0, 2, 1, 1, 0, 1, 2, 2, 1, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = torch.argmax(model(one_hot_test.to(device)), axis=1).detach().cpu().numpy()\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_000</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_001</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_002</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_003</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_004</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>TEST_170</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>TEST_171</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>TEST_172</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>TEST_173</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>TEST_174</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id class\n",
       "0    TEST_000     A\n",
       "1    TEST_001     B\n",
       "2    TEST_002     C\n",
       "3    TEST_003     C\n",
       "4    TEST_004     A\n",
       "..        ...   ...\n",
       "170  TEST_170     B\n",
       "171  TEST_171     C\n",
       "172  TEST_172     C\n",
       "173  TEST_173     B\n",
       "174  TEST_174     B\n",
       "\n",
       "[175 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"submit_high1.csv\")\n",
    "df['class'] = pred\n",
    "df['class'] = df['class'].map(lambda x : 'A' if x==0 else ('B' if x==1 else 'C'))\n",
    "df.to_csv(\"submit_last3.csv\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B    85\n",
       "A    51\n",
       "C    39\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['class'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
