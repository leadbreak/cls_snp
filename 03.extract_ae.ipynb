{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "094661cc-0183-4b8f-a37f-c23061fb166c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22764442-8baf-4fc9-b9dd-95cb0f549361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d198c522-78f4-41af-8247-334ad95d1742",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 41\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed) \n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    \n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "seed_everything(seed=random_seed) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47da1a62-36e6-4d6e-980b-afaea1d29450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((262, 48), (175, 47))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"./data/df_train.csv\")\n",
    "test = pd.read_csv(\"./data/df_test.csv\")\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b154e20-cb18-4357-a9e2-a414696d4c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 1, 0, 2, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 0, 0, 0, 0, 2, 1, 1, 0, 1,\n",
       "        2, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 2, 1, 0, 1, 1, 1, 1, 1, 0, 2, 1, 1, 0,\n",
       "        2, 0, 0, 2, 0, 2, 1, 1, 0, 0, 2, 2, 1, 0, 2, 2, 0, 2, 2, 0, 1, 2, 2, 2,\n",
       "        0, 1, 1, 1, 2, 1, 2, 0, 1, 1, 2, 0, 2, 0, 2, 2, 2, 1, 1, 1, 2, 2, 1, 1,\n",
       "        2, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 2, 2,\n",
       "        1, 1, 2, 0, 0, 1, 1, 0, 2, 2, 2, 0, 1, 2, 0, 1, 2, 0, 2, 1, 1, 2, 2, 1,\n",
       "        1, 1, 2, 2, 0, 1, 0, 2, 2, 2, 1, 0, 1, 1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 0,\n",
       "        0, 2, 2, 0, 0, 2, 2, 2, 2, 0, 2, 0, 0, 2, 0, 2, 2, 1, 1, 2, 1, 2, 2, 1,\n",
       "        1, 1, 0, 1, 1, 2, 1, 2, 0, 1, 0, 1, 0, 2, 2, 1, 2, 1, 2, 1, 0, 0, 1, 2,\n",
       "        0, 0, 2, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 2, 2, 1, 2, 1, 2, 1, 0, 2, 0,\n",
       "        0, 2, 2, 1, 1, 1, 0, 1, 0, 1, 0, 2, 1, 0, 1, 1, 1, 1, 2, 0, 0, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.LongTensor(train['class'].values)\n",
    "X = train.drop(['id', 'class'], axis=1).to_numpy()\n",
    "X_test = test.drop(['id'], axis=1).to_numpy()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2616811e-55d4-4a61-b302-3b5cfedec1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = nn.functional.one_hot(y, num_classes=3).to(device).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "287ba536-93c5-4f23-8c3c-2fcbd63dde34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(437, 46)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = np.concatenate([X, X_test], axis=0)\n",
    "total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec2b6062-e954-49fa-a990-fdfb414ea765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(437, 46)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "total = scaler.fit_transform(total)\n",
    "# total = np.expand_dims(total, axis=1)\n",
    "total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e07b5d53-7f4d-40a5-a5b6-1d9e8b9d8def",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_features, latent_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm0 = nn.Linear(n_features, latent_dim**2)\n",
    "        self.lstm1 = nn.Linear(latent_dim**2, latent_dim*3)        \n",
    "        self.lstm2_1 = nn.Linear(latent_dim*3, 3)\n",
    "        self.lstm2_2 = nn.Linear(latent_dim*3, latent_dim-3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.lstm0(x)\n",
    "        x2 = self.lstm1(x1)\n",
    "        x3_1 = self.lstm2_1(x2)\n",
    "        x3_2 = self.lstm2_2(x2)\n",
    "        \n",
    "        return x3_1, x3_2\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_features, latent_dim):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.lstm0 = nn.Linear(latent_dim, latent_dim*3)\n",
    "        self.lstm1 = nn.Linear(latent_dim*3, latent_dim**2)        \n",
    "        self.lstm2 = nn.Linear(latent_dim**2, latent_dim*2)        \n",
    "        \n",
    "        \n",
    "        self.linear = nn.Linear(in_features=latent_dim*2, out_features=n_features)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        x = self.lstm0(x)\n",
    "        x = self.lstm1(x)\n",
    "        x = self.lstm2(x)\n",
    "        \n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, n_features=46, latent_dim=3+24, device=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder(n_features, latent_dim).to(device)\n",
    "        self.decoder = Decoder(n_features, latent_dim).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1_1, x1_2 = self.encoder(x)\n",
    "        x1 = torch.concat([x1_1, x1_2], axis=1)\n",
    "        x2 = self.decoder(x1)\n",
    "        \n",
    "        return x1, x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f50b900a-e86d-4e30-9d4a-3e8b9e7fee9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoEncoder(\n",
       "  (encoder): Encoder(\n",
       "    (lstm0): Linear(in_features=46, out_features=729, bias=True)\n",
       "    (lstm1): Linear(in_features=729, out_features=81, bias=True)\n",
       "    (lstm2_1): Linear(in_features=81, out_features=3, bias=True)\n",
       "    (lstm2_2): Linear(in_features=81, out_features=24, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (lstm0): Linear(in_features=27, out_features=81, bias=True)\n",
       "    (lstm1): Linear(in_features=81, out_features=729, bias=True)\n",
       "    (lstm2): Linear(in_features=729, out_features=54, bias=True)\n",
       "    (linear): Linear(in_features=54, out_features=46, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_model = AutoEncoder().to(device)\n",
    "encoder_model.load_state_dict(torch.load(f'./models/AutoEncoder_total.pt', map_location=device))\n",
    "encoder_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10606fa2-0c81-4c54-aa79-89ef978e1f25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e681c2bb2014cae8922d5c52dd89cd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/437 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "437 437\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TensorDataset(torch.from_numpy(total).type(torch.float), torch.zeros(len(total)).type(torch.float))\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False, num_workers=2, worker_init_fn=seed_worker)\n",
    "\n",
    "encodings = []\n",
    "errors = []\n",
    "criterion = nn.MSELoss().to(device)\n",
    "for x, label in tqdm(iter(train_loader)):\n",
    "    x = x.to(device)\n",
    "    \n",
    "    encoded_features, decoded_features = encoder_model(x)\n",
    "    encodings += encoded_features.detach().cpu().numpy().tolist()\n",
    "    \n",
    "    loss = criterion(x, decoded_features)\n",
    "    errors.append(loss.detach().cpu())\n",
    "    \n",
    "print(len(encodings), len(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c303a98-ecdd-4302-ad3d-104a61b8f495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(437, 28)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings = np.array(encodings)\n",
    "errors = np.expand_dims(np.array(errors), axis=1)\n",
    "values = np.concatenate([encodings, errors], axis=1)\n",
    "values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96daef73-f313-4242-8717-4b101fa12033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encodings_0</th>\n",
       "      <th>encodings_1</th>\n",
       "      <th>encodings_2</th>\n",
       "      <th>encodings_3</th>\n",
       "      <th>encodings_4</th>\n",
       "      <th>encodings_5</th>\n",
       "      <th>encodings_6</th>\n",
       "      <th>encodings_7</th>\n",
       "      <th>encodings_8</th>\n",
       "      <th>encodings_9</th>\n",
       "      <th>...</th>\n",
       "      <th>encodings_18</th>\n",
       "      <th>encodings_19</th>\n",
       "      <th>encodings_20</th>\n",
       "      <th>encodings_21</th>\n",
       "      <th>encodings_22</th>\n",
       "      <th>encodings_23</th>\n",
       "      <th>encodings_24</th>\n",
       "      <th>encodings_25</th>\n",
       "      <th>encodings_26</th>\n",
       "      <th>errors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.235218</td>\n",
       "      <td>4.172423</td>\n",
       "      <td>-3.853053</td>\n",
       "      <td>-1.533935</td>\n",
       "      <td>2.644730</td>\n",
       "      <td>3.716698</td>\n",
       "      <td>-0.299315</td>\n",
       "      <td>-2.650086</td>\n",
       "      <td>0.783824</td>\n",
       "      <td>-0.406989</td>\n",
       "      <td>...</td>\n",
       "      <td>1.082240</td>\n",
       "      <td>-1.530903</td>\n",
       "      <td>-1.215420</td>\n",
       "      <td>-3.295104</td>\n",
       "      <td>0.245303</td>\n",
       "      <td>-0.055747</td>\n",
       "      <td>-1.089265</td>\n",
       "      <td>-1.649269</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.008067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.164804</td>\n",
       "      <td>6.937546</td>\n",
       "      <td>-0.615424</td>\n",
       "      <td>0.798070</td>\n",
       "      <td>-2.365728</td>\n",
       "      <td>-1.246152</td>\n",
       "      <td>0.571691</td>\n",
       "      <td>0.340047</td>\n",
       "      <td>-0.374678</td>\n",
       "      <td>2.935323</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.065099</td>\n",
       "      <td>-0.072708</td>\n",
       "      <td>-2.343938</td>\n",
       "      <td>0.866952</td>\n",
       "      <td>-0.382145</td>\n",
       "      <td>-2.748686</td>\n",
       "      <td>0.459386</td>\n",
       "      <td>0.983944</td>\n",
       "      <td>3.129371</td>\n",
       "      <td>0.009998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.450719</td>\n",
       "      <td>6.478036</td>\n",
       "      <td>-4.994792</td>\n",
       "      <td>-1.083578</td>\n",
       "      <td>-1.050726</td>\n",
       "      <td>-0.028033</td>\n",
       "      <td>-1.456105</td>\n",
       "      <td>-1.262530</td>\n",
       "      <td>-1.064003</td>\n",
       "      <td>2.599940</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.403145</td>\n",
       "      <td>-1.100375</td>\n",
       "      <td>-1.358432</td>\n",
       "      <td>-1.019482</td>\n",
       "      <td>3.274151</td>\n",
       "      <td>3.080497</td>\n",
       "      <td>2.445034</td>\n",
       "      <td>1.530065</td>\n",
       "      <td>0.137097</td>\n",
       "      <td>0.040302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-5.078038</td>\n",
       "      <td>10.407413</td>\n",
       "      <td>-5.961476</td>\n",
       "      <td>-0.954541</td>\n",
       "      <td>-2.045539</td>\n",
       "      <td>0.913142</td>\n",
       "      <td>3.408923</td>\n",
       "      <td>-2.241139</td>\n",
       "      <td>1.254574</td>\n",
       "      <td>1.749235</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.824590</td>\n",
       "      <td>1.962076</td>\n",
       "      <td>-0.140499</td>\n",
       "      <td>-1.557903</td>\n",
       "      <td>3.796697</td>\n",
       "      <td>3.281870</td>\n",
       "      <td>3.378306</td>\n",
       "      <td>1.271765</td>\n",
       "      <td>-2.542374</td>\n",
       "      <td>0.008206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-4.685338</td>\n",
       "      <td>6.908415</td>\n",
       "      <td>-5.594373</td>\n",
       "      <td>1.017355</td>\n",
       "      <td>-2.475780</td>\n",
       "      <td>1.170744</td>\n",
       "      <td>-1.731986</td>\n",
       "      <td>2.448224</td>\n",
       "      <td>-0.242145</td>\n",
       "      <td>-0.236407</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.392161</td>\n",
       "      <td>-1.289728</td>\n",
       "      <td>-2.297753</td>\n",
       "      <td>2.706107</td>\n",
       "      <td>0.366826</td>\n",
       "      <td>1.816569</td>\n",
       "      <td>1.143190</td>\n",
       "      <td>4.631185</td>\n",
       "      <td>-0.518664</td>\n",
       "      <td>0.036693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>-6.713977</td>\n",
       "      <td>4.797306</td>\n",
       "      <td>-6.859674</td>\n",
       "      <td>-0.427648</td>\n",
       "      <td>-1.465041</td>\n",
       "      <td>0.584409</td>\n",
       "      <td>-1.570887</td>\n",
       "      <td>0.707021</td>\n",
       "      <td>0.202451</td>\n",
       "      <td>-1.051615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.948983</td>\n",
       "      <td>-1.454844</td>\n",
       "      <td>3.104134</td>\n",
       "      <td>-1.628868</td>\n",
       "      <td>1.587918</td>\n",
       "      <td>-1.379632</td>\n",
       "      <td>-0.742597</td>\n",
       "      <td>3.711644</td>\n",
       "      <td>0.533880</td>\n",
       "      <td>0.028781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>-3.505607</td>\n",
       "      <td>6.129325</td>\n",
       "      <td>-2.930341</td>\n",
       "      <td>-0.359151</td>\n",
       "      <td>1.557368</td>\n",
       "      <td>0.367298</td>\n",
       "      <td>1.640623</td>\n",
       "      <td>2.402381</td>\n",
       "      <td>-2.167556</td>\n",
       "      <td>0.352788</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.493095</td>\n",
       "      <td>0.081309</td>\n",
       "      <td>-3.520641</td>\n",
       "      <td>-0.392295</td>\n",
       "      <td>2.330890</td>\n",
       "      <td>1.521016</td>\n",
       "      <td>0.899050</td>\n",
       "      <td>4.925332</td>\n",
       "      <td>0.413980</td>\n",
       "      <td>0.015013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>-1.853786</td>\n",
       "      <td>3.641998</td>\n",
       "      <td>0.470991</td>\n",
       "      <td>-1.175941</td>\n",
       "      <td>3.441618</td>\n",
       "      <td>1.693989</td>\n",
       "      <td>2.112146</td>\n",
       "      <td>1.143281</td>\n",
       "      <td>-0.975709</td>\n",
       "      <td>-2.530986</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.952293</td>\n",
       "      <td>1.092013</td>\n",
       "      <td>-1.333903</td>\n",
       "      <td>1.238227</td>\n",
       "      <td>1.457066</td>\n",
       "      <td>0.588651</td>\n",
       "      <td>-1.395687</td>\n",
       "      <td>1.455708</td>\n",
       "      <td>-0.818322</td>\n",
       "      <td>0.050879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>-3.249345</td>\n",
       "      <td>8.500051</td>\n",
       "      <td>-4.339647</td>\n",
       "      <td>-2.227715</td>\n",
       "      <td>-1.511902</td>\n",
       "      <td>0.807777</td>\n",
       "      <td>-0.969590</td>\n",
       "      <td>-1.554576</td>\n",
       "      <td>0.884828</td>\n",
       "      <td>-0.540825</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.068143</td>\n",
       "      <td>-3.087835</td>\n",
       "      <td>0.714141</td>\n",
       "      <td>0.378201</td>\n",
       "      <td>2.740462</td>\n",
       "      <td>-0.029870</td>\n",
       "      <td>-0.452864</td>\n",
       "      <td>-0.030688</td>\n",
       "      <td>2.088013</td>\n",
       "      <td>0.007352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>-7.493045</td>\n",
       "      <td>6.019004</td>\n",
       "      <td>-9.174811</td>\n",
       "      <td>0.258502</td>\n",
       "      <td>-2.122859</td>\n",
       "      <td>-1.390015</td>\n",
       "      <td>-0.537476</td>\n",
       "      <td>-2.462971</td>\n",
       "      <td>1.481490</td>\n",
       "      <td>-1.179641</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000996</td>\n",
       "      <td>-2.302604</td>\n",
       "      <td>1.013478</td>\n",
       "      <td>-2.525301</td>\n",
       "      <td>0.816626</td>\n",
       "      <td>1.031304</td>\n",
       "      <td>1.218161</td>\n",
       "      <td>2.438878</td>\n",
       "      <td>-1.005346</td>\n",
       "      <td>0.041755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>437 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     encodings_0  encodings_1  encodings_2  encodings_3  encodings_4  \\\n",
       "0      -3.235218     4.172423    -3.853053    -1.533935     2.644730   \n",
       "1      -3.164804     6.937546    -0.615424     0.798070    -2.365728   \n",
       "2      -3.450719     6.478036    -4.994792    -1.083578    -1.050726   \n",
       "3      -5.078038    10.407413    -5.961476    -0.954541    -2.045539   \n",
       "4      -4.685338     6.908415    -5.594373     1.017355    -2.475780   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "432    -6.713977     4.797306    -6.859674    -0.427648    -1.465041   \n",
       "433    -3.505607     6.129325    -2.930341    -0.359151     1.557368   \n",
       "434    -1.853786     3.641998     0.470991    -1.175941     3.441618   \n",
       "435    -3.249345     8.500051    -4.339647    -2.227715    -1.511902   \n",
       "436    -7.493045     6.019004    -9.174811     0.258502    -2.122859   \n",
       "\n",
       "     encodings_5  encodings_6  encodings_7  encodings_8  encodings_9  ...  \\\n",
       "0       3.716698    -0.299315    -2.650086     0.783824    -0.406989  ...   \n",
       "1      -1.246152     0.571691     0.340047    -0.374678     2.935323  ...   \n",
       "2      -0.028033    -1.456105    -1.262530    -1.064003     2.599940  ...   \n",
       "3       0.913142     3.408923    -2.241139     1.254574     1.749235  ...   \n",
       "4       1.170744    -1.731986     2.448224    -0.242145    -0.236407  ...   \n",
       "..           ...          ...          ...          ...          ...  ...   \n",
       "432     0.584409    -1.570887     0.707021     0.202451    -1.051615  ...   \n",
       "433     0.367298     1.640623     2.402381    -2.167556     0.352788  ...   \n",
       "434     1.693989     2.112146     1.143281    -0.975709    -2.530986  ...   \n",
       "435     0.807777    -0.969590    -1.554576     0.884828    -0.540825  ...   \n",
       "436    -1.390015    -0.537476    -2.462971     1.481490    -1.179641  ...   \n",
       "\n",
       "     encodings_18  encodings_19  encodings_20  encodings_21  encodings_22  \\\n",
       "0        1.082240     -1.530903     -1.215420     -3.295104      0.245303   \n",
       "1       -0.065099     -0.072708     -2.343938      0.866952     -0.382145   \n",
       "2       -3.403145     -1.100375     -1.358432     -1.019482      3.274151   \n",
       "3       -0.824590      1.962076     -0.140499     -1.557903      3.796697   \n",
       "4       -2.392161     -1.289728     -2.297753      2.706107      0.366826   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "432      0.948983     -1.454844      3.104134     -1.628868      1.587918   \n",
       "433     -3.493095      0.081309     -3.520641     -0.392295      2.330890   \n",
       "434     -1.952293      1.092013     -1.333903      1.238227      1.457066   \n",
       "435     -0.068143     -3.087835      0.714141      0.378201      2.740462   \n",
       "436      2.000996     -2.302604      1.013478     -2.525301      0.816626   \n",
       "\n",
       "     encodings_23  encodings_24  encodings_25  encodings_26    errors  \n",
       "0       -0.055747     -1.089265     -1.649269      0.001300  0.008067  \n",
       "1       -2.748686      0.459386      0.983944      3.129371  0.009998  \n",
       "2        3.080497      2.445034      1.530065      0.137097  0.040302  \n",
       "3        3.281870      3.378306      1.271765     -2.542374  0.008206  \n",
       "4        1.816569      1.143190      4.631185     -0.518664  0.036693  \n",
       "..            ...           ...           ...           ...       ...  \n",
       "432     -1.379632     -0.742597      3.711644      0.533880  0.028781  \n",
       "433      1.521016      0.899050      4.925332      0.413980  0.015013  \n",
       "434      0.588651     -1.395687      1.455708     -0.818322  0.050879  \n",
       "435     -0.029870     -0.452864     -0.030688      2.088013  0.007352  \n",
       "436      1.031304      1.218161      2.438878     -1.005346  0.041755  \n",
       "\n",
       "[437 rows x 28 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=values, columns=[\"encodings_\"+str(x) for x in range(len(encodings[0]))] + [\"errors\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38e03966-bb96-4ad8-92b4-11b945b36a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./data/ae_values.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085476db-99f1-44ba-94db-fec52555b76b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
