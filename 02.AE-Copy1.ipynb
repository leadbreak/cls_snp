{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8da5a778-128c-4d84-ae25-586932bf9842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, sampler\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de763fbb-6ace-45cd-935f-30dd6b0a54fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "474b1ef8-50a2-4228-8b09-58a9468f4d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 41\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed) \n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    \n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "seed_everything(seed=random_seed) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b6cb23f-c9fe-4a1e-8410-3fdc2b59646e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((262, 18), (175, 18))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"./data/df_train2.csv\")\n",
    "test = pd.read_csv(\"./data/df_test2.csv\")\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb5d0a7f-9cee-4772-943b-142965934278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 1, 0, 2, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 0, 0, 0, 0, 2, 1, 1, 0, 1,\n",
       "        2, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 2, 1, 0, 1, 1, 1, 1, 1, 0, 2, 1, 1, 0,\n",
       "        2, 0, 0, 2, 0, 2, 1, 1, 0, 0, 2, 2, 1, 0, 2, 2, 0, 2, 2, 0, 1, 2, 2, 2,\n",
       "        0, 1, 1, 1, 2, 1, 2, 0, 1, 1, 2, 0, 2, 0, 2, 2, 2, 1, 1, 1, 2, 2, 1, 1,\n",
       "        2, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 0, 2, 2,\n",
       "        1, 1, 2, 0, 0, 1, 1, 0, 2, 2, 2, 0, 1, 2, 0, 1, 2, 0, 2, 1, 1, 2, 2, 1,\n",
       "        1, 1, 2, 2, 0, 1, 0, 2, 2, 2, 1, 0, 1, 1, 1, 1, 1, 0, 1, 2, 1, 1, 1, 0,\n",
       "        0, 2, 2, 0, 0, 2, 2, 2, 2, 0, 2, 0, 0, 2, 0, 2, 2, 1, 1, 2, 1, 2, 2, 1,\n",
       "        1, 1, 0, 1, 1, 2, 1, 2, 0, 1, 0, 1, 0, 2, 2, 1, 2, 1, 2, 1, 0, 0, 1, 2,\n",
       "        0, 0, 2, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 2, 2, 1, 2, 1, 2, 1, 0, 2, 0,\n",
       "        0, 2, 2, 1, 1, 1, 0, 1, 0, 1, 0, 2, 1, 0, 1, 1, 1, 1, 2, 0, 0, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.LongTensor(train['class'].values)\n",
    "X = train.drop(['id', 'class'], axis=1).to_numpy()\n",
    "X_test = test.drop(['id', 'class'], axis=1).to_numpy()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c64f73fd-cbd8-48f4-b1a0-fad1f77e00c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=random_seed)\n",
    "X, y = smote.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7ec9a90-06c6-4e35-9e42-93ea2613630e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = nn.functional.one_hot(torch.LongTensor(y), num_classes=3).to(device).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14ba246f-2991-4c64-a5d8-474a63ab40d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(517, 16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = np.concatenate([X, X_test], axis=0)\n",
    "total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca0db974-ebf3-4762-9b46-9fac884fc76a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(517, 16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "total = scaler.fit_transform(total)\n",
    "# total = np.expand_dims(total, axis=1)\n",
    "total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b35cb587-3a24-45a7-a457-2f2e1b478678",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_features, latent_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm0 = nn.Linear(n_features, latent_dim**3)\n",
    "        self.lstm1 = nn.Linear(latent_dim**3, latent_dim*3)        \n",
    "        self.lstm2_1 = nn.Linear(latent_dim*3, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.lstm0(x)\n",
    "        x2 = self.lstm1(x1)\n",
    "        x3_1 = self.lstm2_1(x2)\n",
    "        \n",
    "        return x3_1\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_features, latent_dim):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.lstm0 = nn.Linear(latent_dim, latent_dim*3)\n",
    "        self.lstm1 = nn.Linear(latent_dim*3, latent_dim**3)        \n",
    "        self.lstm2 = nn.Linear(latent_dim**3, latent_dim*2)        \n",
    "        \n",
    "        \n",
    "        self.linear = nn.Linear(in_features=latent_dim*2, out_features=n_features+3)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        x = self.lstm0(x)\n",
    "        x = self.lstm1(x)\n",
    "        x = self.lstm2(x)\n",
    "        \n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, n_features=16, latent_dim=8, device='cuda:3'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder(n_features, latent_dim).to(device)\n",
    "        self.decoder = Decoder(n_features, latent_dim).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.encoder(x)\n",
    "        x2 = self.decoder(x1)\n",
    "        \n",
    "        return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b9279d5-2548-4a1a-bb96-8ad6375fb209",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'EPOCHS':500,\n",
    "    'LEARNING_RATE':0.0017,\n",
    "    'BATCH_SIZE':517,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29399ab4-cf33-46ab-8632-80908f9bf5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, criterion, test_loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    model_preds = []\n",
    "    true_labels = []\n",
    "    \n",
    "    val_loss = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, label in iter(test_loader):\n",
    "            x, label = x.to(device), label.to(device)\n",
    "\n",
    "            model_pred = model(x)\n",
    "\n",
    "            loss = criterion(model_pred, label)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "\n",
    "    return np.mean(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e51394c-d83c-46ba-b032-b84a454a15b3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1], Loss1 : 6.666666, Loss2 : 1.034303, Train Loss : [7.70097] \n",
      "Epoch [2], Loss1 : 6.666666, Loss2 : 1.034303, Train Loss : [7.70097] \n",
      "Epoch [3], Loss1 : 6.666666, Loss2 : 1.033926, Train Loss : [7.70059] \n",
      "Epoch [4], Loss1 : 6.666666, Loss2 : 1.033174, Train Loss : [7.69984] \n",
      "Epoch [5], Loss1 : 6.666666, Loss2 : 1.032054, Train Loss : [7.69872] \n",
      "Epoch [6], Loss1 : 6.666666, Loss2 : 1.030574, Train Loss : [7.69724] \n",
      "Epoch [7], Loss1 : 6.666666, Loss2 : 1.028744, Train Loss : [7.69541] \n",
      "Epoch [8], Loss1 : 6.666666, Loss2 : 1.026577, Train Loss : [7.69324] \n",
      "Epoch [9], Loss1 : 6.666666, Loss2 : 1.024081, Train Loss : [7.69075] \n",
      "Epoch [10], Loss1 : 6.666666, Loss2 : 1.021269, Train Loss : [7.68793] \n",
      "Epoch [11], Loss1 : 6.666666, Loss2 : 1.018145, Train Loss : [7.68481] \n",
      "Epoch [12], Loss1 : 6.666666, Loss2 : 1.014710, Train Loss : [7.68138] \n",
      "Epoch [13], Loss1 : 6.666666, Loss2 : 1.010958, Train Loss : [7.67762] \n",
      "Epoch [14], Loss1 : 6.666666, Loss2 : 1.006874, Train Loss : [7.67354] \n",
      "Epoch [15], Loss1 : 6.666666, Loss2 : 1.002430, Train Loss : [7.66910] \n",
      "Epoch [16], Loss1 : 6.666666, Loss2 : 0.997589, Train Loss : [7.66426] \n",
      "Epoch [17], Loss1 : 6.666666, Loss2 : 0.992301, Train Loss : [7.65897] \n",
      "Epoch [18], Loss1 : 6.666666, Loss2 : 0.986501, Train Loss : [7.65317] \n",
      "Epoch [19], Loss1 : 6.695907, Loss2 : 0.980114, Train Loss : [7.67602] \n",
      "Epoch [20], Loss1 : 6.666666, Loss2 : 0.973049, Train Loss : [7.63972] \n",
      "Epoch [21], Loss1 : 6.666666, Loss2 : 0.965206, Train Loss : [7.63187] \n",
      "Epoch [22], Loss1 : 6.578947, Loss2 : 0.956473, Train Loss : [7.53542] \n",
      "Epoch [23], Loss1 : 6.608187, Loss2 : 0.946732, Train Loss : [7.55492] \n",
      "Epoch [24], Loss1 : 6.549707, Loss2 : 0.935860, Train Loss : [7.48557] \n",
      "Epoch [25], Loss1 : 6.783626, Loss2 : 0.923737, Train Loss : [7.70736] \n",
      "Epoch [26], Loss1 : 6.578947, Loss2 : 0.910248, Train Loss : [7.48920] \n",
      "Epoch [27], Loss1 : 7.105264, Loss2 : 0.895299, Train Loss : [8.00056] \n",
      "Epoch [28], Loss1 : 7.134503, Loss2 : 0.878825, Train Loss : [8.01333] \n",
      "Epoch [29], Loss1 : 6.637426, Loss2 : 0.860806, Train Loss : [7.49823] \n",
      "Epoch [30], Loss1 : 6.520468, Loss2 : 0.841291, Train Loss : [7.36176] \n",
      "Epoch [31], Loss1 : 6.403509, Loss2 : 0.820422, Train Loss : [7.22393] \n",
      "Epoch [32], Loss1 : 6.666666, Loss2 : 0.798460, Train Loss : [7.46513] \n",
      "Epoch [33], Loss1 : 6.754386, Loss2 : 0.775818, Train Loss : [7.53020] \n",
      "Epoch [34], Loss1 : 6.725146, Loss2 : 0.753077, Train Loss : [7.47822] \n",
      "Epoch [35], Loss1 : 6.959064, Loss2 : 0.730981, Train Loss : [7.69005] \n",
      "Epoch [36], Loss1 : 6.842105, Loss2 : 0.710374, Train Loss : [7.55248] \n",
      "Epoch [37], Loss1 : 6.608187, Loss2 : 0.692047, Train Loss : [7.30023] \n",
      "Epoch [38], Loss1 : 6.695907, Loss2 : 0.676496, Train Loss : [7.37240] \n",
      "Epoch [39], Loss1 : 6.783626, Loss2 : 0.663615, Train Loss : [7.44724] \n",
      "Epoch [40], Loss1 : 6.929824, Loss2 : 0.652502, Train Loss : [7.58233] \n",
      "Epoch [41], Loss1 : 7.046783, Loss2 : 0.641615, Train Loss : [7.68840] \n",
      "Epoch [42], Loss1 : 6.491228, Loss2 : 0.629405, Train Loss : [7.12063] \n",
      "Epoch [43], Loss1 : 7.046783, Loss2 : 0.615061, Train Loss : [7.66184] \n",
      "Epoch [44], Loss1 : 6.461988, Loss2 : 0.598879, Train Loss : [7.06087] \n",
      "Epoch [45], Loss1 : 6.403509, Loss2 : 0.582057, Train Loss : [6.98557] \n",
      "Epoch [46], Loss1 : 6.783626, Loss2 : 0.566117, Train Loss : [7.34974] \n",
      "Epoch [47], Loss1 : 6.754386, Loss2 : 0.552277, Train Loss : [7.30666] \n",
      "Epoch [48], Loss1 : 6.198831, Loss2 : 0.541043, Train Loss : [6.73987] \n",
      "Epoch [49], Loss1 : 6.695907, Loss2 : 0.532178, Train Loss : [7.22808] \n",
      "Epoch [50], Loss1 : 6.754386, Loss2 : 0.524933, Train Loss : [7.27932] \n",
      "Epoch [51], Loss1 : 6.549707, Loss2 : 0.518396, Train Loss : [7.06810] \n",
      "Epoch [52], Loss1 : 6.871345, Loss2 : 0.511816, Train Loss : [7.38316] \n",
      "Epoch [53], Loss1 : 6.637426, Loss2 : 0.504784, Train Loss : [7.14221] \n",
      "Epoch [54], Loss1 : 6.666666, Loss2 : 0.497226, Train Loss : [7.16389] \n",
      "Epoch [55], Loss1 : 7.017544, Loss2 : 0.489274, Train Loss : [7.50682] \n",
      "Epoch [56], Loss1 : 6.491228, Loss2 : 0.481120, Train Loss : [6.97235] \n",
      "Epoch [57], Loss1 : 6.783626, Loss2 : 0.472969, Train Loss : [7.25659] \n",
      "Epoch [58], Loss1 : 6.432748, Loss2 : 0.465061, Train Loss : [6.89781] \n",
      "Epoch [59], Loss1 : 6.871345, Loss2 : 0.457697, Train Loss : [7.32904] \n",
      "Epoch [60], Loss1 : 6.198831, Loss2 : 0.451137, Train Loss : [6.64997] \n",
      "Epoch [61], Loss1 : 6.520468, Loss2 : 0.445417, Train Loss : [6.96589] \n",
      "Epoch [62], Loss1 : 6.403509, Loss2 : 0.440221, Train Loss : [6.84373] \n",
      "Epoch [63], Loss1 : 6.432748, Loss2 : 0.435042, Train Loss : [6.86779] \n",
      "Epoch [64], Loss1 : 6.461988, Loss2 : 0.429561, Train Loss : [6.89155] \n",
      "Epoch [65], Loss1 : 6.432748, Loss2 : 0.423865, Train Loss : [6.85661] \n",
      "Epoch [66], Loss1 : 6.403509, Loss2 : 0.418287, Train Loss : [6.82180] \n",
      "Epoch [67], Loss1 : 6.812866, Loss2 : 0.413043, Train Loss : [7.22591] \n",
      "Epoch [68], Loss1 : 6.929824, Loss2 : 0.408018, Train Loss : [7.33784] \n",
      "Epoch [69], Loss1 : 6.725146, Loss2 : 0.402862, Train Loss : [7.12801] \n",
      "Epoch [70], Loss1 : 6.988304, Loss2 : 0.397286, Train Loss : [7.38559] \n",
      "Epoch [71], Loss1 : 6.578947, Loss2 : 0.391282, Train Loss : [6.97023] \n",
      "Epoch [72], Loss1 : 6.315789, Loss2 : 0.385130, Train Loss : [6.70092] \n",
      "Epoch [73], Loss1 : 6.491228, Loss2 : 0.379209, Train Loss : [6.87044] \n",
      "Epoch [74], Loss1 : 6.520468, Loss2 : 0.373757, Train Loss : [6.89422] \n",
      "Epoch [75], Loss1 : 6.695907, Loss2 : 0.368746, Train Loss : [7.06465] \n",
      "Epoch [76], Loss1 : 6.754386, Loss2 : 0.363952, Train Loss : [7.11834] \n",
      "Epoch [77], Loss1 : 6.491228, Loss2 : 0.359171, Train Loss : [6.85040] \n",
      "Epoch [78], Loss1 : 6.725146, Loss2 : 0.354384, Train Loss : [7.07953] \n",
      "Epoch [79], Loss1 : 6.549707, Loss2 : 0.349719, Train Loss : [6.89943] \n",
      "Epoch [80], Loss1 : 6.929824, Loss2 : 0.345291, Train Loss : [7.27512] \n",
      "Epoch [81], Loss1 : 6.491228, Loss2 : 0.341078, Train Loss : [6.83231] \n",
      "Epoch [82], Loss1 : 6.578947, Loss2 : 0.336963, Train Loss : [6.91591] \n",
      "Epoch [83], Loss1 : 6.754386, Loss2 : 0.332875, Train Loss : [7.08726] \n",
      "Epoch [84], Loss1 : 6.783626, Loss2 : 0.328859, Train Loss : [7.11249] \n",
      "Epoch [85], Loss1 : 6.754386, Loss2 : 0.325025, Train Loss : [7.07941] \n",
      "Epoch [86], Loss1 : 6.812866, Loss2 : 0.321431, Train Loss : [7.13430] \n",
      "Epoch [87], Loss1 : 6.608187, Loss2 : 0.318032, Train Loss : [6.92622] \n",
      "Epoch [88], Loss1 : 6.812866, Loss2 : 0.314709, Train Loss : [7.12757] \n",
      "Epoch [89], Loss1 : 6.812866, Loss2 : 0.311368, Train Loss : [7.12423] \n",
      "Epoch [90], Loss1 : 6.637426, Loss2 : 0.308002, Train Loss : [6.94543] \n",
      "Epoch [91], Loss1 : 6.871345, Loss2 : 0.304665, Train Loss : [7.17601] \n",
      "Epoch [92], Loss1 : 6.432748, Loss2 : 0.301390, Train Loss : [6.73414] \n",
      "Epoch [93], Loss1 : 7.163743, Loss2 : 0.298147, Train Loss : [7.46189] \n",
      "Epoch [94], Loss1 : 6.491228, Loss2 : 0.294890, Train Loss : [6.78612] \n",
      "Epoch [95], Loss1 : 6.871345, Loss2 : 0.291657, Train Loss : [7.16300] \n",
      "Epoch [96], Loss1 : 6.286550, Loss2 : 0.288572, Train Loss : [6.57512] \n",
      "Epoch [97], Loss1 : 6.783626, Loss2 : 0.285728, Train Loss : [7.06935] \n",
      "Epoch [98], Loss1 : 6.666666, Loss2 : 0.283100, Train Loss : [6.94977] \n",
      "Epoch [99], Loss1 : 6.754386, Loss2 : 0.280611, Train Loss : [7.03500] \n",
      "Epoch [100], Loss1 : 6.695907, Loss2 : 0.278224, Train Loss : [6.97413] \n",
      "Epoch [101], Loss1 : 6.315789, Loss2 : 0.275899, Train Loss : [6.59169] \n",
      "Epoch [102], Loss1 : 6.140351, Loss2 : 0.273525, Train Loss : [6.41388] \n",
      "Epoch [103], Loss1 : 6.608187, Loss2 : 0.271003, Train Loss : [6.87919] \n",
      "Epoch [104], Loss1 : 6.666666, Loss2 : 0.268345, Train Loss : [6.93501] \n",
      "Epoch [105], Loss1 : 6.637426, Loss2 : 0.265592, Train Loss : [6.90302] \n",
      "Epoch [106], Loss1 : 6.695907, Loss2 : 0.262732, Train Loss : [6.95864] \n",
      "Epoch [107], Loss1 : 6.666666, Loss2 : 0.259751, Train Loss : [6.92642] \n",
      "Epoch [108], Loss1 : 6.315789, Loss2 : 0.256696, Train Loss : [6.57249] \n",
      "Epoch [109], Loss1 : 6.900585, Loss2 : 0.253620, Train Loss : [7.15420] \n",
      "Epoch [110], Loss1 : 6.608187, Loss2 : 0.250532, Train Loss : [6.85872] \n",
      "Epoch [111], Loss1 : 6.461988, Loss2 : 0.247442, Train Loss : [6.70943] \n",
      "Epoch [112], Loss1 : 6.695907, Loss2 : 0.244397, Train Loss : [6.94030] \n",
      "Epoch [113], Loss1 : 6.666666, Loss2 : 0.241455, Train Loss : [6.90812] \n",
      "Epoch [114], Loss1 : 6.374269, Loss2 : 0.238654, Train Loss : [6.61292] \n",
      "Epoch [115], Loss1 : 6.491228, Loss2 : 0.236030, Train Loss : [6.72726] \n",
      "Epoch [116], Loss1 : 6.637426, Loss2 : 0.233625, Train Loss : [6.87105] \n",
      "Epoch [117], Loss1 : 6.637426, Loss2 : 0.231467, Train Loss : [6.86889] \n",
      "Epoch [118], Loss1 : 6.228070, Loss2 : 0.229555, Train Loss : [6.45763] \n",
      "Epoch [119], Loss1 : 6.959064, Loss2 : 0.227870, Train Loss : [7.18693] \n",
      "Epoch [120], Loss1 : 6.754386, Loss2 : 0.226387, Train Loss : [6.98077] \n",
      "Epoch [121], Loss1 : 6.812866, Loss2 : 0.225081, Train Loss : [7.03795] \n",
      "Epoch [122], Loss1 : 6.754386, Loss2 : 0.223926, Train Loss : [6.97831] \n",
      "Epoch [123], Loss1 : 7.076023, Loss2 : 0.222904, Train Loss : [7.29893] \n",
      "Epoch [124], Loss1 : 6.608187, Loss2 : 0.221997, Train Loss : [6.83018] \n",
      "Epoch [125], Loss1 : 6.754386, Loss2 : 0.221183, Train Loss : [6.97557] \n",
      "Epoch [126], Loss1 : 6.725146, Loss2 : 0.220441, Train Loss : [6.94559] \n",
      "Epoch [127], Loss1 : 6.520468, Loss2 : 0.219736, Train Loss : [6.74020] \n",
      "Epoch [128], Loss1 : 6.783626, Loss2 : 0.219019, Train Loss : [7.00265] \n",
      "Epoch [129], Loss1 : 6.315789, Loss2 : 0.218243, Train Loss : [6.53403] \n",
      "Epoch [130], Loss1 : 6.812866, Loss2 : 0.217387, Train Loss : [7.03025] \n",
      "Epoch [131], Loss1 : 6.695907, Loss2 : 0.216459, Train Loss : [6.91237] \n",
      "Epoch [132], Loss1 : 6.345029, Loss2 : 0.215478, Train Loss : [6.56051] \n",
      "Epoch [133], Loss1 : 6.900585, Loss2 : 0.214473, Train Loss : [7.11506] \n",
      "Epoch [134], Loss1 : 6.988304, Loss2 : 0.213473, Train Loss : [7.20178] \n",
      "Epoch [135], Loss1 : 6.725146, Loss2 : 0.212504, Train Loss : [6.93765] \n",
      "Epoch [136], Loss1 : 6.959064, Loss2 : 0.211574, Train Loss : [7.17064] \n",
      "Epoch [137], Loss1 : 6.491228, Loss2 : 0.210685, Train Loss : [6.70191] \n",
      "Epoch [138], Loss1 : 6.637426, Loss2 : 0.209827, Train Loss : [6.84725] \n",
      "Epoch [139], Loss1 : 6.637426, Loss2 : 0.208976, Train Loss : [6.84640] \n",
      "Epoch [140], Loss1 : 6.374269, Loss2 : 0.208104, Train Loss : [6.58237] \n",
      "Epoch [141], Loss1 : 7.251462, Loss2 : 0.207187, Train Loss : [7.45865] \n",
      "Epoch [142], Loss1 : 6.812866, Loss2 : 0.206204, Train Loss : [7.01907] \n",
      "Epoch [143], Loss1 : 6.549707, Loss2 : 0.205139, Train Loss : [6.75485] \n",
      "Epoch [144], Loss1 : 6.900585, Loss2 : 0.203985, Train Loss : [7.10457] \n",
      "Epoch [145], Loss1 : 6.432748, Loss2 : 0.202742, Train Loss : [6.63549] \n",
      "Epoch [146], Loss1 : 6.111111, Loss2 : 0.201417, Train Loss : [6.31253] \n",
      "Epoch [147], Loss1 : 6.959064, Loss2 : 0.200021, Train Loss : [7.15908] \n",
      "Epoch [148], Loss1 : 6.695907, Loss2 : 0.198566, Train Loss : [6.89447] \n",
      "Epoch [149], Loss1 : 6.929824, Loss2 : 0.197065, Train Loss : [7.12689] \n",
      "Epoch [150], Loss1 : 6.403509, Loss2 : 0.195531, Train Loss : [6.59904] \n",
      "Epoch [151], Loss1 : 6.608187, Loss2 : 0.193973, Train Loss : [6.80216] \n",
      "Epoch [152], Loss1 : 6.637426, Loss2 : 0.192403, Train Loss : [6.82983] \n",
      "Epoch [153], Loss1 : 6.608187, Loss2 : 0.190836, Train Loss : [6.79902] \n",
      "Epoch [154], Loss1 : 6.695907, Loss2 : 0.189296, Train Loss : [6.88520] \n",
      "Epoch [155], Loss1 : 6.959064, Loss2 : 0.187815, Train Loss : [7.14688] \n",
      "Epoch [156], Loss1 : 6.345029, Loss2 : 0.186428, Train Loss : [6.53146] \n",
      "Epoch [157], Loss1 : 6.842105, Loss2 : 0.185176, Train Loss : [7.02728] \n",
      "Epoch [158], Loss1 : 6.461988, Loss2 : 0.184091, Train Loss : [6.64608] \n",
      "Epoch [159], Loss1 : 5.964912, Loss2 : 0.183189, Train Loss : [6.14810] \n",
      "Epoch [160], Loss1 : 6.725146, Loss2 : 0.182467, Train Loss : [6.90761] \n",
      "Epoch [161], Loss1 : 6.754386, Loss2 : 0.181898, Train Loss : [6.93628] \n",
      "Epoch [162], Loss1 : 6.374269, Loss2 : 0.181439, Train Loss : [6.55571] \n",
      "Epoch [163], Loss1 : 6.520468, Loss2 : 0.181042, Train Loss : [6.70151] \n",
      "Epoch [164], Loss1 : 6.695907, Loss2 : 0.180663, Train Loss : [6.87657] \n",
      "Epoch [165], Loss1 : 7.397661, Loss2 : 0.180275, Train Loss : [7.57794] \n",
      "Epoch [166], Loss1 : 6.812866, Loss2 : 0.179866, Train Loss : [6.99273] \n",
      "Epoch [167], Loss1 : 7.134503, Loss2 : 0.179437, Train Loss : [7.31394] \n",
      "Epoch [168], Loss1 : 7.134503, Loss2 : 0.178996, Train Loss : [7.31350] \n",
      "Epoch [169], Loss1 : 6.549707, Loss2 : 0.178554, Train Loss : [6.72826] \n",
      "Epoch [170], Loss1 : 6.812866, Loss2 : 0.178119, Train Loss : [6.99098] \n",
      "Epoch [171], Loss1 : 6.578947, Loss2 : 0.177697, Train Loss : [6.75664] \n",
      "Epoch [172], Loss1 : 6.725146, Loss2 : 0.177292, Train Loss : [6.90244] \n",
      "Epoch [173], Loss1 : 6.783626, Loss2 : 0.176904, Train Loss : [6.96053] \n",
      "Epoch [174], Loss1 : 6.783626, Loss2 : 0.176531, Train Loss : [6.96016] \n",
      "Epoch [175], Loss1 : 6.549707, Loss2 : 0.176170, Train Loss : [6.72588] \n",
      "Epoch [176], Loss1 : 6.842105, Loss2 : 0.175819, Train Loss : [7.01792] \n",
      "Epoch [177], Loss1 : 6.403509, Loss2 : 0.175473, Train Loss : [6.57898] \n",
      "Epoch [178], Loss1 : 6.637426, Loss2 : 0.175131, Train Loss : [6.81256] \n",
      "Epoch [179], Loss1 : 6.754386, Loss2 : 0.174792, Train Loss : [6.92918] \n",
      "Epoch [180], Loss1 : 6.169591, Loss2 : 0.174458, Train Loss : [6.34405] \n",
      "Epoch [181], Loss1 : 6.491228, Loss2 : 0.174133, Train Loss : [6.66536] \n",
      "Epoch [182], Loss1 : 6.900585, Loss2 : 0.173820, Train Loss : [7.07440] \n",
      "Epoch [183], Loss1 : 6.374269, Loss2 : 0.173521, Train Loss : [6.54779] \n",
      "Epoch [184], Loss1 : 6.637426, Loss2 : 0.173236, Train Loss : [6.81066] \n",
      "Epoch [185], Loss1 : 6.578947, Loss2 : 0.172963, Train Loss : [6.75191] \n",
      "Epoch [186], Loss1 : 6.959064, Loss2 : 0.172699, Train Loss : [7.13176] \n",
      "Epoch [187], Loss1 : 6.578947, Loss2 : 0.172441, Train Loss : [6.75139] \n",
      "Epoch [188], Loss1 : 6.695907, Loss2 : 0.172185, Train Loss : [6.86809] \n",
      "Epoch [189], Loss1 : 6.228070, Loss2 : 0.171931, Train Loss : [6.40000] \n",
      "Epoch [190], Loss1 : 6.988304, Loss2 : 0.171678, Train Loss : [7.15998] \n",
      "Epoch [191], Loss1 : 6.695907, Loss2 : 0.171428, Train Loss : [6.86733] \n",
      "Epoch [192], Loss1 : 6.929824, Loss2 : 0.171180, Train Loss : [7.10100] \n",
      "Epoch [193], Loss1 : 6.812866, Loss2 : 0.170936, Train Loss : [6.98380] \n",
      "Epoch [194], Loss1 : 6.783626, Loss2 : 0.170694, Train Loss : [6.95432] \n",
      "Epoch [195], Loss1 : 6.023392, Loss2 : 0.170456, Train Loss : [6.19385] \n",
      "Epoch [196], Loss1 : 6.608187, Loss2 : 0.170221, Train Loss : [6.77841] \n",
      "Epoch [197], Loss1 : 6.666666, Loss2 : 0.169990, Train Loss : [6.83666] \n",
      "Epoch [198], Loss1 : 6.666666, Loss2 : 0.169766, Train Loss : [6.83643] \n",
      "Epoch [199], Loss1 : 6.695907, Loss2 : 0.169548, Train Loss : [6.86545] \n",
      "Epoch [200], Loss1 : 6.812866, Loss2 : 0.169337, Train Loss : [6.98220] \n",
      "Epoch [201], Loss1 : 6.666666, Loss2 : 0.169133, Train Loss : [6.83580] \n",
      "Epoch [202], Loss1 : 6.725146, Loss2 : 0.168933, Train Loss : [6.89408] \n",
      "Epoch [203], Loss1 : 7.017544, Loss2 : 0.168737, Train Loss : [7.18628] \n",
      "Epoch [204], Loss1 : 6.988304, Loss2 : 0.168546, Train Loss : [7.15685] \n",
      "Epoch [205], Loss1 : 6.549707, Loss2 : 0.168359, Train Loss : [6.71807] \n",
      "Epoch [206], Loss1 : 6.637426, Loss2 : 0.168177, Train Loss : [6.80560] \n",
      "Epoch [207], Loss1 : 6.520468, Loss2 : 0.168001, Train Loss : [6.68847] \n",
      "Epoch [208], Loss1 : 6.959064, Loss2 : 0.167831, Train Loss : [7.12689] \n",
      "Epoch [209], Loss1 : 6.754386, Loss2 : 0.167667, Train Loss : [6.92205] \n",
      "Epoch [210], Loss1 : 6.666666, Loss2 : 0.167508, Train Loss : [6.83417] \n",
      "Epoch [211], Loss1 : 6.520468, Loss2 : 0.167355, Train Loss : [6.68782] \n",
      "Epoch [212], Loss1 : 6.578947, Loss2 : 0.167206, Train Loss : [6.74615] \n",
      "Epoch [213], Loss1 : 6.578947, Loss2 : 0.167062, Train Loss : [6.74601] \n",
      "Epoch [214], Loss1 : 6.666666, Loss2 : 0.166922, Train Loss : [6.83359] \n",
      "Epoch [215], Loss1 : 6.140351, Loss2 : 0.166787, Train Loss : [6.30714] \n",
      "Epoch [216], Loss1 : 6.842105, Loss2 : 0.166657, Train Loss : [7.00876] \n",
      "Epoch [217], Loss1 : 6.695907, Loss2 : 0.166532, Train Loss : [6.86244] \n",
      "Epoch [218], Loss1 : 6.812866, Loss2 : 0.166411, Train Loss : [6.97928] \n",
      "Epoch [219], Loss1 : 6.929824, Loss2 : 0.166295, Train Loss : [7.09612] \n",
      "Epoch [220], Loss1 : 6.666666, Loss2 : 0.166183, Train Loss : [6.83285] \n",
      "Epoch [221], Loss1 : 6.461988, Loss2 : 0.166076, Train Loss : [6.62806] \n",
      "Epoch [222], Loss1 : 6.432748, Loss2 : 0.165973, Train Loss : [6.59872] \n",
      "Epoch [223], Loss1 : 6.637426, Loss2 : 0.165874, Train Loss : [6.80330] \n",
      "Epoch [224], Loss1 : 6.491228, Loss2 : 0.165779, Train Loss : [6.65701] \n",
      "Epoch [225], Loss1 : 6.959064, Loss2 : 0.165687, Train Loss : [7.12475] \n",
      "Epoch [226], Loss1 : 7.251462, Loss2 : 0.165598, Train Loss : [7.41706] \n",
      "Epoch [227], Loss1 : 6.666666, Loss2 : 0.165513, Train Loss : [6.83218] \n",
      "Epoch [228], Loss1 : 6.198831, Loss2 : 0.165430, Train Loss : [6.36426] \n",
      "Epoch [229], Loss1 : 6.461988, Loss2 : 0.165350, Train Loss : [6.62734] \n",
      "Epoch [230], Loss1 : 6.812866, Loss2 : 0.165274, Train Loss : [6.97814] \n",
      "Epoch [231], Loss1 : 6.783626, Loss2 : 0.165200, Train Loss : [6.94883] \n",
      "Epoch [232], Loss1 : 7.105264, Loss2 : 0.165128, Train Loss : [7.27039] \n",
      "Epoch [233], Loss1 : 6.461988, Loss2 : 0.165058, Train Loss : [6.62705] \n",
      "Epoch [234], Loss1 : 6.637426, Loss2 : 0.164991, Train Loss : [6.80242] \n",
      "Epoch [235], Loss1 : 6.812866, Loss2 : 0.164926, Train Loss : [6.97779] \n",
      "Epoch [236], Loss1 : 6.228070, Loss2 : 0.164862, Train Loss : [6.39293] \n",
      "Epoch [237], Loss1 : 6.754386, Loss2 : 0.164801, Train Loss : [6.91919] \n",
      "Epoch [238], Loss1 : 6.959064, Loss2 : 0.164741, Train Loss : [7.12381] \n",
      "Epoch [239], Loss1 : 6.666666, Loss2 : 0.164682, Train Loss : [6.83135] \n",
      "Epoch [240], Loss1 : 6.637426, Loss2 : 0.164626, Train Loss : [6.80205] \n",
      "Epoch [241], Loss1 : 6.783626, Loss2 : 0.164570, Train Loss : [6.94820] \n",
      "Epoch [242], Loss1 : 6.929824, Loss2 : 0.164516, Train Loss : [7.09434] \n",
      "Epoch [243], Loss1 : 6.783626, Loss2 : 0.164463, Train Loss : [6.94809] \n",
      "Epoch [244], Loss1 : 6.637426, Loss2 : 0.164411, Train Loss : [6.80184] \n",
      "Epoch [245], Loss1 : 6.432748, Loss2 : 0.164360, Train Loss : [6.59711] \n",
      "Epoch [246], Loss1 : 7.017544, Loss2 : 0.164310, Train Loss : [7.18185] \n",
      "Epoch [247], Loss1 : 6.461988, Loss2 : 0.164261, Train Loss : [6.62625] \n",
      "Epoch [248], Loss1 : 6.374269, Loss2 : 0.164212, Train Loss : [6.53848] \n",
      "Epoch [249], Loss1 : 6.228070, Loss2 : 0.164165, Train Loss : [6.39223] \n",
      "Epoch [250], Loss1 : 6.754386, Loss2 : 0.164118, Train Loss : [6.91850] \n",
      "Epoch [251], Loss1 : 6.725146, Loss2 : 0.164072, Train Loss : [6.88922] \n",
      "Epoch [252], Loss1 : 6.578947, Loss2 : 0.164026, Train Loss : [6.74297] \n",
      "Epoch [253], Loss1 : 6.812866, Loss2 : 0.163981, Train Loss : [6.97685] \n",
      "Epoch [254], Loss1 : 6.695907, Loss2 : 0.163937, Train Loss : [6.85984] \n",
      "Epoch [255], Loss1 : 6.491228, Loss2 : 0.163894, Train Loss : [6.65512] \n",
      "Epoch [256], Loss1 : 6.520468, Loss2 : 0.163851, Train Loss : [6.68432] \n",
      "Epoch [257], Loss1 : 6.666666, Loss2 : 0.163809, Train Loss : [6.83047] \n",
      "Epoch [258], Loss1 : 6.988304, Loss2 : 0.163767, Train Loss : [7.15207] \n",
      "Epoch [259], Loss1 : 6.900585, Loss2 : 0.163726, Train Loss : [7.06431] \n",
      "Epoch [260], Loss1 : 6.461988, Loss2 : 0.163686, Train Loss : [6.62567] \n",
      "Epoch [261], Loss1 : 6.608187, Loss2 : 0.163647, Train Loss : [6.77183] \n",
      "Epoch [262], Loss1 : 6.461988, Loss2 : 0.163608, Train Loss : [6.62560] \n",
      "Epoch [263], Loss1 : 6.052631, Loss2 : 0.163569, Train Loss : [6.21620] \n",
      "Epoch [264], Loss1 : 6.871345, Loss2 : 0.163531, Train Loss : [7.03488] \n",
      "Epoch [265], Loss1 : 6.432748, Loss2 : 0.163494, Train Loss : [6.59624] \n",
      "Epoch [266], Loss1 : 6.725146, Loss2 : 0.163458, Train Loss : [6.88860] \n",
      "Epoch [267], Loss1 : 6.871345, Loss2 : 0.163422, Train Loss : [7.03477] \n",
      "Epoch [268], Loss1 : 6.637426, Loss2 : 0.163386, Train Loss : [6.80081] \n",
      "Epoch [269], Loss1 : 6.578947, Loss2 : 0.163352, Train Loss : [6.74230] \n",
      "Epoch [270], Loss1 : 6.988304, Loss2 : 0.163318, Train Loss : [7.15162] \n",
      "Epoch [271], Loss1 : 7.251462, Loss2 : 0.163285, Train Loss : [7.41475] \n",
      "Epoch [272], Loss1 : 6.520468, Loss2 : 0.163252, Train Loss : [6.68372] \n",
      "Epoch [273], Loss1 : 6.374269, Loss2 : 0.163220, Train Loss : [6.53749] \n",
      "Epoch [274], Loss1 : 6.754386, Loss2 : 0.163189, Train Loss : [6.91757] \n",
      "Epoch [275], Loss1 : 6.929824, Loss2 : 0.163158, Train Loss : [7.09298] \n",
      "Epoch [276], Loss1 : 6.666666, Loss2 : 0.163128, Train Loss : [6.82979] \n",
      "Epoch [277], Loss1 : 7.046783, Loss2 : 0.163099, Train Loss : [7.20988] \n",
      "Epoch [278], Loss1 : 6.140351, Loss2 : 0.163071, Train Loss : [6.30342] \n",
      "Epoch [279], Loss1 : 7.163743, Loss2 : 0.163044, Train Loss : [7.32679] \n",
      "Epoch [280], Loss1 : 6.783626, Loss2 : 0.163017, Train Loss : [6.94664] \n",
      "Epoch [281], Loss1 : 6.403509, Loss2 : 0.162991, Train Loss : [6.56650] \n",
      "Epoch [282], Loss1 : 7.222222, Loss2 : 0.162966, Train Loss : [7.38519] \n",
      "Epoch [283], Loss1 : 6.842105, Loss2 : 0.162941, Train Loss : [7.00505] \n",
      "Epoch [284], Loss1 : 6.842105, Loss2 : 0.162917, Train Loss : [7.00502] \n",
      "Epoch [285], Loss1 : 7.017544, Loss2 : 0.162894, Train Loss : [7.18044] \n",
      "Epoch [286], Loss1 : 6.783626, Loss2 : 0.162872, Train Loss : [6.94650] \n",
      "Epoch [287], Loss1 : 6.725146, Loss2 : 0.162851, Train Loss : [6.88800] \n",
      "Epoch [288], Loss1 : 6.578947, Loss2 : 0.162830, Train Loss : [6.74178] \n",
      "Epoch [289], Loss1 : 6.023392, Loss2 : 0.162810, Train Loss : [6.18620] \n",
      "Epoch [290], Loss1 : 6.754386, Loss2 : 0.162791, Train Loss : [6.91718] \n",
      "Epoch [291], Loss1 : 6.783626, Loss2 : 0.162773, Train Loss : [6.94640] \n",
      "Epoch [292], Loss1 : 6.637426, Loss2 : 0.162755, Train Loss : [6.80018] \n",
      "Epoch [293], Loss1 : 6.608187, Loss2 : 0.162738, Train Loss : [6.77092] \n",
      "Epoch [294], Loss1 : 6.637426, Loss2 : 0.162721, Train Loss : [6.80015] \n",
      "Epoch [295], Loss1 : 6.637426, Loss2 : 0.162705, Train Loss : [6.80013] \n",
      "Epoch [296], Loss1 : 6.403509, Loss2 : 0.162690, Train Loss : [6.56620] \n",
      "Epoch [297], Loss1 : 7.163743, Loss2 : 0.162676, Train Loss : [7.32642] \n",
      "Epoch [298], Loss1 : 6.783626, Loss2 : 0.162662, Train Loss : [6.94629] \n",
      "Epoch [299], Loss1 : 6.549707, Loss2 : 0.162649, Train Loss : [6.71236] \n",
      "Epoch [300], Loss1 : 6.549707, Loss2 : 0.162636, Train Loss : [6.71234] \n",
      "Epoch [301], Loss1 : 6.929824, Loss2 : 0.162624, Train Loss : [7.09245] \n",
      "Epoch [302], Loss1 : 6.812866, Loss2 : 0.162612, Train Loss : [6.97548] \n",
      "Epoch [303], Loss1 : 6.461988, Loss2 : 0.162601, Train Loss : [6.62459] \n",
      "Epoch [304], Loss1 : 6.812866, Loss2 : 0.162590, Train Loss : [6.97546] \n",
      "Epoch [305], Loss1 : 6.286550, Loss2 : 0.162580, Train Loss : [6.44913] \n",
      "Epoch [306], Loss1 : 6.959064, Loss2 : 0.162570, Train Loss : [7.12163] \n",
      "Epoch [307], Loss1 : 6.754386, Loss2 : 0.162560, Train Loss : [6.91695] \n",
      "Epoch [308], Loss1 : 6.988304, Loss2 : 0.162551, Train Loss : [7.15085] \n",
      "Epoch [309], Loss1 : 6.842105, Loss2 : 0.162542, Train Loss : [7.00465] \n",
      "Epoch [310], Loss1 : 6.374269, Loss2 : 0.162534, Train Loss : [6.53680] \n",
      "Epoch [311], Loss1 : 6.666666, Loss2 : 0.162526, Train Loss : [6.82919] \n",
      "Epoch [312], Loss1 : 6.725146, Loss2 : 0.162519, Train Loss : [6.88766] \n",
      "Epoch [313], Loss1 : 6.812866, Loss2 : 0.162511, Train Loss : [6.97538] \n",
      "Epoch [314], Loss1 : 7.309942, Loss2 : 0.162504, Train Loss : [7.47245] \n",
      "Epoch [315], Loss1 : 6.637426, Loss2 : 0.162497, Train Loss : [6.79992] \n",
      "Epoch [316], Loss1 : 6.608187, Loss2 : 0.162491, Train Loss : [6.77068] \n",
      "Epoch [317], Loss1 : 7.105264, Loss2 : 0.162485, Train Loss : [7.26775] \n",
      "Epoch [318], Loss1 : 6.754386, Loss2 : 0.162479, Train Loss : [6.91686] \n",
      "Epoch [319], Loss1 : 6.695907, Loss2 : 0.162473, Train Loss : [6.85838] \n",
      "Epoch [320], Loss1 : 6.666666, Loss2 : 0.162467, Train Loss : [6.82913] \n",
      "Epoch [321], Loss1 : 6.812866, Loss2 : 0.162462, Train Loss : [6.97533] \n",
      "Epoch [322], Loss1 : 6.549707, Loss2 : 0.162457, Train Loss : [6.71216] \n",
      "Epoch [323], Loss1 : 6.900585, Loss2 : 0.162452, Train Loss : [7.06304] \n",
      "Epoch [324], Loss1 : 6.374269, Loss2 : 0.162447, Train Loss : [6.53672] \n",
      "Epoch [325], Loss1 : 6.608187, Loss2 : 0.162443, Train Loss : [6.77063] \n",
      "Epoch [326], Loss1 : 7.105264, Loss2 : 0.162438, Train Loss : [7.26770] \n",
      "Epoch [327], Loss1 : 6.695907, Loss2 : 0.162434, Train Loss : [6.85834] \n",
      "Epoch [328], Loss1 : 6.520468, Loss2 : 0.162430, Train Loss : [6.68290] \n",
      "Epoch [329], Loss1 : 6.783626, Loss2 : 0.162426, Train Loss : [6.94605] \n",
      "Epoch [330], Loss1 : 6.549707, Loss2 : 0.162422, Train Loss : [6.71213] \n",
      "Epoch [331], Loss1 : 6.725146, Loss2 : 0.162418, Train Loss : [6.88756] \n",
      "Epoch [332], Loss1 : 6.315789, Loss2 : 0.162415, Train Loss : [6.47820] \n",
      "Epoch [333], Loss1 : 6.959064, Loss2 : 0.162411, Train Loss : [7.12148] \n",
      "Epoch [334], Loss1 : 6.461988, Loss2 : 0.162408, Train Loss : [6.62440] \n",
      "Epoch [335], Loss1 : 6.228070, Loss2 : 0.162405, Train Loss : [6.39048] \n",
      "Epoch [336], Loss1 : 6.549707, Loss2 : 0.162402, Train Loss : [6.71211] \n",
      "Epoch [337], Loss1 : 6.374269, Loss2 : 0.162399, Train Loss : [6.53667] \n",
      "Epoch [338], Loss1 : 6.695907, Loss2 : 0.162396, Train Loss : [6.85830] \n",
      "Epoch [339], Loss1 : 6.374269, Loss2 : 0.162393, Train Loss : [6.53666] \n",
      "Epoch [340], Loss1 : 6.608187, Loss2 : 0.162390, Train Loss : [6.77058] \n",
      "Epoch [341], Loss1 : 6.666666, Loss2 : 0.162388, Train Loss : [6.82905] \n",
      "Epoch [342], Loss1 : 6.315789, Loss2 : 0.162385, Train Loss : [6.47817] \n",
      "Epoch [343], Loss1 : 6.491228, Loss2 : 0.162383, Train Loss : [6.65361] \n",
      "Epoch [344], Loss1 : 6.725146, Loss2 : 0.162380, Train Loss : [6.88753] \n",
      "Epoch [345], Loss1 : 6.520468, Loss2 : 0.162378, Train Loss : [6.68285] \n",
      "Epoch [346], Loss1 : 7.134503, Loss2 : 0.162376, Train Loss : [7.29688] \n",
      "Epoch [347], Loss1 : 6.812866, Loss2 : 0.162374, Train Loss : [6.97524] \n",
      "Epoch [348], Loss1 : 6.608187, Loss2 : 0.162372, Train Loss : [6.77056] \n",
      "Epoch [349], Loss1 : 6.959064, Loss2 : 0.162370, Train Loss : [7.12143] \n",
      "Epoch [350], Loss1 : 6.725146, Loss2 : 0.162368, Train Loss : [6.88751] \n",
      "Epoch [351], Loss1 : 6.812866, Loss2 : 0.162366, Train Loss : [6.97523] \n",
      "Epoch [352], Loss1 : 7.222222, Loss2 : 0.162364, Train Loss : [7.38459] \n",
      "Epoch [353], Loss1 : 6.842105, Loss2 : 0.162362, Train Loss : [7.00447] \n",
      "Epoch [354], Loss1 : 6.608187, Loss2 : 0.162360, Train Loss : [6.77055] \n",
      "Epoch [355], Loss1 : 6.461988, Loss2 : 0.162359, Train Loss : [6.62435] \n",
      "Epoch [356], Loss1 : 6.754386, Loss2 : 0.162357, Train Loss : [6.91674] \n",
      "Epoch [357], Loss1 : 6.783626, Loss2 : 0.162356, Train Loss : [6.94598] \n",
      "Epoch [358], Loss1 : 6.695907, Loss2 : 0.162354, Train Loss : [6.85826] \n",
      "Epoch [359], Loss1 : 6.374269, Loss2 : 0.162353, Train Loss : [6.53662] \n",
      "Epoch [360], Loss1 : 6.812866, Loss2 : 0.162351, Train Loss : [6.97522] \n",
      "Epoch [361], Loss1 : 6.403509, Loss2 : 0.162350, Train Loss : [6.56586] \n",
      "Epoch [362], Loss1 : 6.578947, Loss2 : 0.162348, Train Loss : [6.74130] \n",
      "Epoch [363], Loss1 : 6.549707, Loss2 : 0.162347, Train Loss : [6.71205] \n",
      "Epoch [364], Loss1 : 6.666666, Loss2 : 0.162346, Train Loss : [6.82901] \n",
      "Epoch [365], Loss1 : 6.725146, Loss2 : 0.162345, Train Loss : [6.88749] \n",
      "Epoch [366], Loss1 : 6.578947, Loss2 : 0.162343, Train Loss : [6.74129] \n",
      "Epoch [367], Loss1 : 6.491228, Loss2 : 0.162342, Train Loss : [6.65357] \n",
      "Epoch [368], Loss1 : 6.666666, Loss2 : 0.162341, Train Loss : [6.82901] \n",
      "Epoch [369], Loss1 : 6.549707, Loss2 : 0.162340, Train Loss : [6.71205] \n",
      "Epoch [370], Loss1 : 6.111111, Loss2 : 0.162339, Train Loss : [6.27345] \n",
      "Epoch [371], Loss1 : 6.345029, Loss2 : 0.162338, Train Loss : [6.50737] \n",
      "Epoch [372], Loss1 : 6.461988, Loss2 : 0.162337, Train Loss : [6.62433] \n",
      "Epoch [373], Loss1 : 6.871345, Loss2 : 0.162336, Train Loss : [7.03368] \n",
      "Epoch [374], Loss1 : 6.520468, Loss2 : 0.162335, Train Loss : [6.68280] \n",
      "Epoch [375], Loss1 : 6.608187, Loss2 : 0.162334, Train Loss : [6.77052] \n",
      "Epoch [376], Loss1 : 7.017544, Loss2 : 0.162333, Train Loss : [7.17988] \n",
      "Epoch [377], Loss1 : 6.637426, Loss2 : 0.162332, Train Loss : [6.79976] \n",
      "Epoch [378], Loss1 : 6.754386, Loss2 : 0.162331, Train Loss : [6.91672] \n",
      "Epoch [379], Loss1 : 6.608187, Loss2 : 0.162330, Train Loss : [6.77052] \n",
      "Epoch [380], Loss1 : 6.374269, Loss2 : 0.162330, Train Loss : [6.53660] \n",
      "Epoch [381], Loss1 : 6.403509, Loss2 : 0.162329, Train Loss : [6.56584] \n",
      "Epoch [382], Loss1 : 7.134503, Loss2 : 0.162328, Train Loss : [7.29683] \n",
      "Epoch [383], Loss1 : 6.315789, Loss2 : 0.162327, Train Loss : [6.47812] \n",
      "Epoch [384], Loss1 : 6.695907, Loss2 : 0.162327, Train Loss : [6.85823] \n",
      "Epoch [385], Loss1 : 6.140351, Loss2 : 0.162326, Train Loss : [6.30268] \n",
      "Epoch [386], Loss1 : 6.637426, Loss2 : 0.162325, Train Loss : [6.79975] \n",
      "Epoch [387], Loss1 : 6.403509, Loss2 : 0.162325, Train Loss : [6.56583] \n",
      "Epoch [388], Loss1 : 6.549707, Loss2 : 0.162324, Train Loss : [6.71203] \n",
      "Epoch [389], Loss1 : 6.812866, Loss2 : 0.162323, Train Loss : [6.97519] \n",
      "Epoch [390], Loss1 : 6.345029, Loss2 : 0.162323, Train Loss : [6.50735] \n",
      "Epoch [391], Loss1 : 6.169591, Loss2 : 0.162322, Train Loss : [6.33191] \n",
      "Epoch [392], Loss1 : 6.988304, Loss2 : 0.162322, Train Loss : [7.15063] \n",
      "Epoch [393], Loss1 : 6.812866, Loss2 : 0.162321, Train Loss : [6.97519] \n",
      "Epoch [394], Loss1 : 6.929824, Loss2 : 0.162320, Train Loss : [7.09214] \n",
      "Epoch [395], Loss1 : 6.959064, Loss2 : 0.162320, Train Loss : [7.12138] \n",
      "Epoch [396], Loss1 : 7.134503, Loss2 : 0.162319, Train Loss : [7.29682] \n",
      "Epoch [397], Loss1 : 6.754386, Loss2 : 0.162319, Train Loss : [6.91670] \n",
      "Epoch [398], Loss1 : 6.374269, Loss2 : 0.162318, Train Loss : [6.53659] \n",
      "Epoch [399], Loss1 : 6.374269, Loss2 : 0.162318, Train Loss : [6.53659] \n",
      "Epoch [400], Loss1 : 6.608187, Loss2 : 0.162317, Train Loss : [6.77050] \n",
      "Epoch [401], Loss1 : 6.754386, Loss2 : 0.162317, Train Loss : [6.91670] \n",
      "Epoch [402], Loss1 : 6.520468, Loss2 : 0.162317, Train Loss : [6.68278] \n",
      "Epoch [403], Loss1 : 6.578947, Loss2 : 0.162316, Train Loss : [6.74126] \n",
      "Epoch [404], Loss1 : 6.725146, Loss2 : 0.162316, Train Loss : [6.88746] \n",
      "Epoch [405], Loss1 : 6.374269, Loss2 : 0.162315, Train Loss : [6.53658] \n",
      "Epoch [406], Loss1 : 6.812866, Loss2 : 0.162315, Train Loss : [6.97518] \n",
      "Epoch [407], Loss1 : 6.695907, Loss2 : 0.162315, Train Loss : [6.85822] \n",
      "Epoch [408], Loss1 : 6.666666, Loss2 : 0.162314, Train Loss : [6.82898] \n",
      "Epoch [409], Loss1 : 6.608187, Loss2 : 0.162314, Train Loss : [6.77050] \n",
      "Epoch [410], Loss1 : 6.783626, Loss2 : 0.162313, Train Loss : [6.94594] \n",
      "Epoch [411], Loss1 : 6.666666, Loss2 : 0.162313, Train Loss : [6.82898] \n",
      "Epoch [412], Loss1 : 6.754386, Loss2 : 0.162313, Train Loss : [6.91670] \n",
      "Epoch [413], Loss1 : 6.286550, Loss2 : 0.162312, Train Loss : [6.44886] \n",
      "Epoch [414], Loss1 : 6.695907, Loss2 : 0.162312, Train Loss : [6.85822] \n",
      "Epoch [415], Loss1 : 6.461988, Loss2 : 0.162312, Train Loss : [6.62430] \n",
      "Epoch [416], Loss1 : 6.988304, Loss2 : 0.162312, Train Loss : [7.15062] \n",
      "Epoch [417], Loss1 : 6.929824, Loss2 : 0.162311, Train Loss : [7.09214] \n",
      "Epoch [418], Loss1 : 6.637426, Loss2 : 0.162311, Train Loss : [6.79974] \n",
      "Epoch [419], Loss1 : 6.520468, Loss2 : 0.162311, Train Loss : [6.68278] \n",
      "Epoch [420], Loss1 : 6.549707, Loss2 : 0.162310, Train Loss : [6.71202] \n",
      "Epoch [421], Loss1 : 6.842105, Loss2 : 0.162310, Train Loss : [7.00442] \n",
      "Epoch [422], Loss1 : 7.046783, Loss2 : 0.162310, Train Loss : [7.20909] \n",
      "Epoch [423], Loss1 : 7.339181, Loss2 : 0.162310, Train Loss : [7.50149] \n",
      "Epoch [424], Loss1 : 6.520468, Loss2 : 0.162309, Train Loss : [6.68278] \n",
      "Epoch [425], Loss1 : 6.432748, Loss2 : 0.162309, Train Loss : [6.59506] \n",
      "Epoch [426], Loss1 : 6.754386, Loss2 : 0.162309, Train Loss : [6.91670] \n",
      "Epoch [427], Loss1 : 6.871345, Loss2 : 0.162309, Train Loss : [7.03365] \n",
      "Epoch [428], Loss1 : 6.637426, Loss2 : 0.162309, Train Loss : [6.79974] \n",
      "Epoch [429], Loss1 : 6.871345, Loss2 : 0.162308, Train Loss : [7.03365] \n",
      "Epoch [430], Loss1 : 6.637426, Loss2 : 0.162308, Train Loss : [6.79973] \n",
      "Epoch [431], Loss1 : 6.929824, Loss2 : 0.162308, Train Loss : [7.09213] \n",
      "Epoch [432], Loss1 : 6.783626, Loss2 : 0.162308, Train Loss : [6.94593] \n",
      "Epoch [433], Loss1 : 6.812866, Loss2 : 0.162308, Train Loss : [6.97517] \n",
      "Epoch [434], Loss1 : 6.871345, Loss2 : 0.162307, Train Loss : [7.03365] \n",
      "Epoch [435], Loss1 : 7.397661, Loss2 : 0.162307, Train Loss : [7.55997] \n",
      "Epoch [436], Loss1 : 6.959064, Loss2 : 0.162307, Train Loss : [7.12137] \n",
      "Epoch [437], Loss1 : 6.637426, Loss2 : 0.162307, Train Loss : [6.79973] \n",
      "Epoch [438], Loss1 : 6.140351, Loss2 : 0.162307, Train Loss : [6.30266] \n",
      "Epoch [439], Loss1 : 7.280702, Loss2 : 0.162307, Train Loss : [7.44301] \n",
      "Epoch [440], Loss1 : 6.842105, Loss2 : 0.162306, Train Loss : [7.00441] \n",
      "Epoch [441], Loss1 : 6.578947, Loss2 : 0.162306, Train Loss : [6.74125] \n",
      "Epoch [442], Loss1 : 6.549707, Loss2 : 0.162306, Train Loss : [6.71201] \n",
      "Epoch [443], Loss1 : 6.988304, Loss2 : 0.162306, Train Loss : [7.15061] \n",
      "Epoch [444], Loss1 : 7.046783, Loss2 : 0.162306, Train Loss : [7.20909] \n",
      "Epoch [445], Loss1 : 6.578947, Loss2 : 0.162306, Train Loss : [6.74125] \n",
      "Epoch [446], Loss1 : 6.549707, Loss2 : 0.162306, Train Loss : [6.71201] \n",
      "Epoch [447], Loss1 : 6.374269, Loss2 : 0.162306, Train Loss : [6.53657] \n",
      "Epoch [448], Loss1 : 6.871345, Loss2 : 0.162305, Train Loss : [7.03365] \n",
      "Epoch [449], Loss1 : 6.812866, Loss2 : 0.162305, Train Loss : [6.97517] \n",
      "Epoch [450], Loss1 : 6.695907, Loss2 : 0.162305, Train Loss : [6.85821] \n",
      "Epoch [451], Loss1 : 6.695907, Loss2 : 0.162305, Train Loss : [6.85821] \n",
      "Epoch [452], Loss1 : 6.549707, Loss2 : 0.162305, Train Loss : [6.71201] \n",
      "Epoch [453], Loss1 : 6.637426, Loss2 : 0.162305, Train Loss : [6.79973] \n",
      "Epoch [454], Loss1 : 6.725146, Loss2 : 0.162305, Train Loss : [6.88745] \n",
      "Epoch [455], Loss1 : 6.491228, Loss2 : 0.162305, Train Loss : [6.65353] \n",
      "Epoch [456], Loss1 : 6.169591, Loss2 : 0.162305, Train Loss : [6.33190] \n",
      "Epoch [457], Loss1 : 6.783626, Loss2 : 0.162305, Train Loss : [6.94593] \n",
      "Epoch [458], Loss1 : 6.900585, Loss2 : 0.162305, Train Loss : [7.06289] \n",
      "Epoch [459], Loss1 : 6.666666, Loss2 : 0.162304, Train Loss : [6.82897] \n",
      "Epoch [460], Loss1 : 7.046783, Loss2 : 0.162304, Train Loss : [7.20909] \n",
      "Epoch [461], Loss1 : 6.637426, Loss2 : 0.162304, Train Loss : [6.79973] \n",
      "Epoch [462], Loss1 : 6.666666, Loss2 : 0.162304, Train Loss : [6.82897] \n",
      "Epoch [463], Loss1 : 6.842105, Loss2 : 0.162304, Train Loss : [7.00441] \n",
      "Epoch [464], Loss1 : 6.695907, Loss2 : 0.162304, Train Loss : [6.85821] \n",
      "Epoch [465], Loss1 : 6.461988, Loss2 : 0.162304, Train Loss : [6.62429] \n",
      "Epoch [466], Loss1 : 7.076023, Loss2 : 0.162304, Train Loss : [7.23833] \n",
      "Epoch [467], Loss1 : 6.491228, Loss2 : 0.162304, Train Loss : [6.65353] \n",
      "Epoch [468], Loss1 : 6.403509, Loss2 : 0.162304, Train Loss : [6.56581] \n",
      "Epoch [469], Loss1 : 6.637426, Loss2 : 0.162304, Train Loss : [6.79973] \n",
      "Epoch [470], Loss1 : 6.666666, Loss2 : 0.162304, Train Loss : [6.82897] \n",
      "Epoch [471], Loss1 : 6.228070, Loss2 : 0.162304, Train Loss : [6.39037] \n",
      "Epoch [472], Loss1 : 6.783626, Loss2 : 0.162304, Train Loss : [6.94593] \n",
      "Epoch [473], Loss1 : 6.345029, Loss2 : 0.162304, Train Loss : [6.50733] \n",
      "Epoch [474], Loss1 : 6.812866, Loss2 : 0.162304, Train Loss : [6.97517] \n",
      "Epoch [475], Loss1 : 6.520468, Loss2 : 0.162304, Train Loss : [6.68277] \n",
      "Epoch [476], Loss1 : 6.549707, Loss2 : 0.162304, Train Loss : [6.71201] \n",
      "Epoch [477], Loss1 : 6.695907, Loss2 : 0.162304, Train Loss : [6.85821] \n",
      "Epoch [478], Loss1 : 6.549707, Loss2 : 0.162304, Train Loss : [6.71201] \n",
      "Epoch [479], Loss1 : 6.637426, Loss2 : 0.162304, Train Loss : [6.79973] \n",
      "Epoch [480], Loss1 : 6.812866, Loss2 : 0.162304, Train Loss : [6.97517] \n",
      "Epoch [481], Loss1 : 6.549707, Loss2 : 0.162304, Train Loss : [6.71201] \n",
      "Epoch [482], Loss1 : 7.163743, Loss2 : 0.162304, Train Loss : [7.32605] \n",
      "Epoch [483], Loss1 : 6.754386, Loss2 : 0.162304, Train Loss : [6.91669] \n",
      "Epoch [484], Loss1 : 6.432748, Loss2 : 0.162304, Train Loss : [6.59505] \n",
      "Epoch [485], Loss1 : 6.637426, Loss2 : 0.162304, Train Loss : [6.79973] \n",
      "Epoch [486], Loss1 : 6.578947, Loss2 : 0.162304, Train Loss : [6.74125] \n",
      "Epoch [487], Loss1 : 6.725146, Loss2 : 0.162304, Train Loss : [6.88745] \n",
      "Epoch [488], Loss1 : 6.140351, Loss2 : 0.162304, Train Loss : [6.30265] \n",
      "Epoch [489], Loss1 : 6.783626, Loss2 : 0.162304, Train Loss : [6.94593] \n",
      "Epoch [490], Loss1 : 6.959064, Loss2 : 0.162304, Train Loss : [7.12137] \n",
      "Epoch [491], Loss1 : 6.695907, Loss2 : 0.162304, Train Loss : [6.85821] \n",
      "Epoch [492], Loss1 : 6.345029, Loss2 : 0.162304, Train Loss : [6.50733] \n",
      "Epoch [493], Loss1 : 7.192983, Loss2 : 0.162304, Train Loss : [7.35529] \n",
      "Epoch [494], Loss1 : 6.374269, Loss2 : 0.162304, Train Loss : [6.53657] \n",
      "Epoch [495], Loss1 : 6.461988, Loss2 : 0.162304, Train Loss : [6.62429] \n",
      "Epoch [496], Loss1 : 6.929824, Loss2 : 0.162304, Train Loss : [7.09213] \n",
      "Epoch [497], Loss1 : 7.017544, Loss2 : 0.162304, Train Loss : [7.17985] \n",
      "Epoch [498], Loss1 : 6.929824, Loss2 : 0.162304, Train Loss : [7.09213] \n",
      "Epoch [499], Loss1 : 6.725146, Loss2 : 0.162304, Train Loss : [6.88745] \n",
      "Epoch [500], Loss1 : 6.871345, Loss2 : 0.162304, Train Loss : [7.03365] "
     ]
    }
   ],
   "source": [
    "# DataLoader 정의\n",
    "train_dataset = TensorDataset(torch.from_numpy(total).type(torch.float), torch.from_numpy(total).type(torch.float))\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True, num_workers=2, worker_init_fn=seed_worker)\n",
    "\n",
    "# 학습 모델 설정\n",
    "model = AutoEncoder().to(device)\n",
    "\n",
    "criterion1 = nn.CrossEntropyLoss().to(device)\n",
    "criterion2 = nn.MSELoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=CFG['LEARNING_RATE'])  # Adam\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=len(train_loader) * int(CFG['EPOCHS']*0.5),\n",
    "    num_training_steps=len(train_loader) * CFG['EPOCHS']\n",
    ")\n",
    "\n",
    "\n",
    "best_score = 0.3\n",
    "\n",
    "# train\n",
    "torch.backends.cudnn.benchmark = True\n",
    "for epoch in range(1,CFG['EPOCHS']+1):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    for x, label in iter(train_loader):            \n",
    "        x, label = x.to(device), label.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred_vector = model(x)\n",
    "        # target = pred_label[:len(y2), :3] # 앞의 세 개를 label로 유도\n",
    "        # target2 = torch.softmax(target, axis=1)    \n",
    "        \n",
    "        target1 = torch.argmax(y2.float(), axis=1)\n",
    "        target2 = torch.argmax( torch.softmax(pred_vector[:len(y2), :3], axis=1), axis=1)\n",
    "\n",
    "        acc = torch.sum(target1 == target2) / len(target1)\n",
    "        \n",
    "        loss1 = (1 - acc) * 10\n",
    "        loss2 = criterion2(label, pred_vector[:, 3:])\n",
    "        \n",
    "        loss = loss1 + loss2\n",
    "        # loss = loss1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "    tr_loss = np.mean(train_loss)\n",
    "    print()\n",
    "    print(f'Epoch [{epoch}], Loss1 : {loss1:.6f}, Loss2 : {loss2:.6f}, Train Loss : [{tr_loss:.5f}]', end=\" \")\n",
    "    # print(f'Epoch [{epoch}], Train Loss : [{tr_loss:.5f}]')\n",
    "\n",
    "    if best_score > tr_loss:\n",
    "        print(\"- Model Saved!\")\n",
    "        torch.save(model.state_dict(), f'./models/AutoEncoder_total2.pt')\n",
    "        best_score = tr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd4e7d1f-2966-4f06-bfed-4086f687f05e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        ...,\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.]], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0465df1-cd88-49ee-b2d4-a7f8b612781f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3304, device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target1 = torch.argmax(y2.float(), axis=1)\n",
    "target2 = torch.argmax( torch.softmax(pred_vector[:len(y2), :3], axis=1), axis=1)\n",
    "\n",
    "torch.sum(target1 == target2) / len(target1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c999c40e-603b-4c72-be57-5732f093e38c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 0, 1, 1, 1, 0, 0, 2, 1, 1, 2, 1, 1, 0, 0, 0, 2, 0, 1, 2, 2, 1,\n",
       "        1, 1, 1, 0, 0, 1, 0, 1, 2, 0, 1, 0, 2, 0, 0, 2, 1, 0, 0, 1, 1, 0, 2, 0,\n",
       "        1, 1, 0, 0, 2, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "        2, 1, 1, 1, 0, 2, 1, 2, 1, 0, 1, 1, 2, 1, 1, 2, 1, 0, 1, 0, 1, 1, 0, 1,\n",
       "        0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 2, 2, 0, 1, 2, 0,\n",
       "        1, 0, 0, 0, 0, 1, 0, 1, 2, 1, 0, 1, 1, 2, 1, 0, 1, 0, 2, 0, 1, 0, 1, 0,\n",
       "        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 2, 0, 1, 1, 1, 0, 0, 1,\n",
       "        0, 1, 0, 0, 1, 2, 1, 0, 1, 0, 2, 1, 0, 1, 0, 0, 0, 0, 2, 1, 1, 0, 1, 0,\n",
       "        1, 2, 0, 1, 0, 0, 1, 0, 1, 1, 0, 2, 2, 1, 0, 0, 2, 1, 1, 1, 1, 1, 1, 2,\n",
       "        1, 2, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 2, 1, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,\n",
       "        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 2, 0, 0, 2, 1, 1, 2, 0, 2, 2, 1, 1, 1, 1,\n",
       "        1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 2, 0, 0, 1, 1, 0, 1, 1, 2, 0,\n",
       "        0, 0, 0, 1, 1, 1, 2, 0, 0, 1, 2, 0, 2, 1, 1, 1, 2, 1, 1, 1, 0, 0, 2, 0,\n",
       "        2, 1, 1, 1, 1, 0], device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax( torch.softmax(pred_vector[:len(y2), :3], axis=1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f77b5d5-9ec0-4a0c-908d-ae37e9817cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pred_label, pred_vector = model(x)\n",
    "target = pred_label[:len(y2), :3] # 앞의 세 개를 label로 유도\n",
    "target2 = torch.softmax(target, axis=1) \n",
    "\"\"\"\n",
    "\n",
    "pred_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb8d766-91fb-4b0e-81e9-848b950df5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label[:len(y2), :3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2579dbd-bfa0-4842-8739-b1b4c1df458d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.softmax(target, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4908ebb-da00-4e97-ab7f-537baa369f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.softmax(target, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ec42d7-bd07-4d09-ae60-c9808f00d477",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
