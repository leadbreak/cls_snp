{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18c6179f-70ad-4c8b-9f97-12d12ea3c6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "import os, warnings, pickle, gzip\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRFClassifier, XGBRFRegressor, XGBRegressor, XGBClassifier\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17ad766e-effd-4b60-84cf-96ffe409ced0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoModel :\n",
    "\n",
    "    \n",
    "    def __init__(self,\n",
    "                #  trainPath, \n",
    "                #  testPath,\n",
    "                 scaling_cols:list=None,\n",
    "                 gpu_option: bool=False,\n",
    "                 gpu_id : str=None,\n",
    "                 name:str=datetime.now().strftime('%Y-%m-%d %H:%M:%S').replace(\" \",\"_\").replace(\"-\",\"\").replace(\":\",\"\"),\n",
    "                 timeLen:int=1) :\n",
    "\n",
    "        if gpu_option == True :\n",
    "            from tensorflow.python.client import device_lib\n",
    "            \n",
    "            os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "            os.environ[\"CUDA_VISIBLE_DEVICES\"]= gpu_id\n",
    "            \n",
    "            print()\n",
    "            print(\"=\"*100)\n",
    "            gpu_num = len(device_lib.list_local_devices()[1:])\n",
    "            print(f\"\\n사용할 gpu 수 : {gpu_num}\\n\")\n",
    "            print(\"=\"*100)\n",
    "\n",
    "        self.name = name\n",
    "        self.save_dir = os.path.join(\"./results\", self.name)\n",
    "        print(f\"작업 경로 : {self.save_dir}\")\n",
    "        if not os.path.isdir(self.save_dir) :\n",
    "            if not os.path.isdir(\"./results\") :\n",
    "                os.mkdir(\"./results\")\n",
    "            os.mkdir(self.save_dir) \n",
    "    \n",
    "    def tuning_xgbc(self, \n",
    "                       trainX, trainY,\n",
    "                       testX=None, testY=None,\n",
    "                       n_trials:int=100) :\n",
    "\n",
    "            self.X = trainX\n",
    "            self.Y = trainY\n",
    "            if testX is not None :\n",
    "                self.testX = testX\n",
    "                self.testY = testY\n",
    "            else :\n",
    "                self.testX = trainX\n",
    "                self.testY = trainY\n",
    "\n",
    "\n",
    "            def objective(trial) :\n",
    "\n",
    "                params = {'grow_policy' : trial.suggest_categorical('grow_policy', ['depthwise']),\n",
    "                          'n_estimators' : trial.suggest_int('n_estimators', 10, 500, step=10),\n",
    "                          'num_parallel_tree' : trial.suggest_categorical('num_parallel_tree', [1, 3]),\n",
    "                          'max_depth' : trial.suggest_categorical('max_depth', [0, 4, 8, 15, 20]),\n",
    "                          'learning_rate' : trial.suggest_categorical('learning_rate', [0.1, 0.2, 0.3, 0.4, 0.5]),\n",
    "                          'gamma' : trial.suggest_float('gamma', 0, 0.5, step=0.01), \n",
    "                          'min_child_weight' : trial.suggest_int('min_child_weight', 0, 10),\n",
    "                          'subsample' : trial.suggest_float('subsample', 0.1, 1, step=0.1),\n",
    "                          'colsample_bytree' : trial.suggest_float('colsample_bytree', 0.5, 1, step=0.1),\n",
    "                          'reg_alpha' : trial.suggest_categorical('reg_alpha', [0.0001, 0.001, 0.01, 0.1, 0, 1, 10, 100] ),\n",
    "                          'reg_lambda' : trial.suggest_categorical('reg_lambda', [0.0001, 0.001, 0.01, 0.1, 0, 1, 10, 100] ),\n",
    "                          'max_delta_step' : trial.suggest_float('max_delta_step', 0, 10, step=0.1),\n",
    "                          'tree_method' : trial.suggest_categorical('tree_method', ['gpu_hist']), \n",
    "                          'predictor' : trial.suggest_categorical('predictor', ['gpu_predictor']), \n",
    "                          'n_jobs' : trial.suggest_categorical('n_jobs', [-1]), \n",
    "                            }\n",
    "\n",
    "                booster_param = trial.suggest_categorical('booster', ['gbtree', 'gblinear', 'dart'])\n",
    "\n",
    "                if booster_param == 'dart' :   \n",
    "                    params['rate_drop'] = trial.suggest_float('rate_drop', 0.1, 0.9, step=0.1)\n",
    "\n",
    "\n",
    "                classifier_obj = XGBClassifier(booster=booster_param, **params)     \n",
    "\n",
    "                # scores = sklearn.model_selection.cross_val_score(classifier_obj, self.X, self.Y, scoring='f1', n_jobs=-1, cv=3)\n",
    "                pred = classifier_obj.fit(self.X, self.Y).predict(self.testX)\n",
    "                score = f1_score(self.testY, pred, average='macro')\n",
    "                return score\n",
    "\n",
    "            sampler = optuna.samplers.TPESampler(seed=42)\n",
    "            study = optuna.create_study(\n",
    "                                        study_name='xgbc_parameter_opt',                                    \n",
    "                                        direction=\"maximize\", \n",
    "                                        sampler=sampler)\n",
    "            study.optimize(objective, n_trials=n_trials, n_jobs=-1)\n",
    "            print(\"Best Score:\", study.best_value)\n",
    "            print(\"Best trial:\", study.best_trial.params) \n",
    "\n",
    "            return study\n",
    "\n",
    "    def tuning_lgbc(self, \n",
    "                   trainX, trainY,\n",
    "                   testX=None, testY=None,\n",
    "                   n_trials:int=100) :\n",
    "\n",
    "        self.X = trainX\n",
    "        self.Y = trainY\n",
    "        if testX is not None :\n",
    "            self.testX = testX\n",
    "            self.testY = testY\n",
    "        else :\n",
    "            self.testX = trainX\n",
    "            self.testY = trainY\n",
    "\n",
    "\n",
    "        def objective(trial) :\n",
    "\n",
    "            params = {'grow_policy' : trial.suggest_categorical('grow_policy', ['lossguide']),\n",
    "                      'n_estimators' : trial.suggest_int('n_estimators', 10, 500, step=10),\n",
    "                      'max_depth' : trial.suggest_categorical('max_depth', [0, 8, 15, 20, 30]),\n",
    "                      'num_parallel_tree' : trial.suggest_categorical('num_parallel_tree', [1, 3]),\n",
    "                      'learning_rate' : trial.suggest_categorical('learning_rate', [0.1, 0.2, 0.3, 0.4, 0.5]),\n",
    "                      'gamma' : trial.suggest_float('gamma', 0, 0.5, step=0.01), \n",
    "                      'min_child_weight' : trial.suggest_int('min_child_weight', 0, 10),\n",
    "                      'subsample' : trial.suggest_float('subsample', 0.1, 1, step=0.1),\n",
    "                      'colsample_bytree' : trial.suggest_float('colsample_bytree', 0.5, 1, step=0.1),\n",
    "                      'reg_alpha' : trial.suggest_categorical('reg_alpha', [0.0001, 0.001, 0.01, 0.1, 0, 1, 10, 100] ),\n",
    "                      'reg_lambda' : trial.suggest_categorical('reg_lambda', [0.0001, 0.001, 0.01, 0.1, 0, 1, 10, 100] ),\n",
    "                      'max_delta_step' : trial.suggest_float('max_delta_step', 0, 10, step=0.1),\n",
    "                      'tree_method' : trial.suggest_categorical('tree_method', ['gpu_hist']), \n",
    "                      'predictor' : trial.suggest_categorical('predictor', ['gpu_predictor']), \n",
    "                      'n_jobs' : trial.suggest_categorical('n_jobs', [-1]), \n",
    "                        }\n",
    "\n",
    "            classifier_obj = XGBClassifier(**params)     \n",
    "\n",
    "            # scores = sklearn.model_selection.cross_val_score(classifier_obj, self.X, self.Y, scoring='f1', n_jobs=-1, cv=3)\n",
    "            pred = classifier_obj.fit(self.X, self.Y).predict(self.testX)\n",
    "            score = f1_score(self.testY, pred, average='macro')\n",
    "            return score\n",
    "\n",
    "        sampler = optuna.samplers.TPESampler(seed=42)\n",
    "        study = optuna.create_study(\n",
    "                                    study_name='lgbmc_parameter_opt',                                    \n",
    "                                    direction=\"maximize\", \n",
    "                                    sampler=sampler)\n",
    "        study.optimize(objective, n_trials=n_trials, n_jobs=-1)\n",
    "        print(\"Best Score:\", study.best_value)\n",
    "        print(\"Best trial:\", study.best_trial.params)\n",
    "\n",
    "        return study\n",
    "\n",
    "    def XGBC(self,\n",
    "             X, Y,\n",
    "             training:bool=True,\n",
    "             objective:str='binary:logistic',\n",
    "             num_parallel_tree:int=1,\n",
    "             max_depth:int=6,\n",
    "             learning_rate:float=0.3,\n",
    "             gamma:float=0.0,\n",
    "             min_child_weight:float=0.0,\n",
    "             colsample_bytree:float=1.0,\n",
    "             reg_alpha:float=0.0,\n",
    "             reg_lambda:float=1.0,\n",
    "             max_delta_step:float=0.0,\n",
    "             skip_drop:float=0.0,\n",
    "             booster:str='gbtree',\n",
    "             rate_drop:float=0.5,\n",
    "             subsample:float=1,\n",
    "             grow_policy:str='depthwise',\n",
    "             n_estimators:int=200,\n",
    "             scale_pos_weight:int=1,\n",
    "             tree_method:str='gpu_hist',\n",
    "             predictor:str='gpu_predictor',\n",
    "             n_jobs:int=-1\n",
    "             ) :\n",
    "\n",
    "        print(f\"XGB Classification with {booster} & {grow_policy} is Started!\\n\")\n",
    "        if training==True :    \n",
    "            # Model Training\n",
    "            if (booster == 'dart') & (grow_policy=='depth_wise') :\n",
    "                model = XGBClassifier(objective=objective,\n",
    "                                      n_estimators=n_estimators, \n",
    "                                      num_parallel_tree=num_parallel_tree,\n",
    "                                      booster=booster, \n",
    "                                      max_depth=max_depth,\n",
    "                                      learning_rate=learning_rate,\n",
    "                                      gamma=gamma,\n",
    "                                      min_child_weight=min_child_weight,\n",
    "                                      colsample_bytree=colsample_bytree,\n",
    "                                      reg_alpha=reg_alpha,\n",
    "                                      reg_lambda=reg_lambda,\n",
    "                                      max_delta_step=max_delta_step,\n",
    "                                      skip_drop=skip_drop,                                      \n",
    "                                      rate_drop=rate_drop, \n",
    "                                      grow_policy=grow_policy, \n",
    "                                      subsample=subsample, \n",
    "                                      scale_pos_weight=scale_pos_weight, \n",
    "                                      tree_method=tree_method,                                       \n",
    "                                      predictor=predictor,\n",
    "                                      n_jobs=n_jobs\n",
    "                                      )\n",
    "            else :\n",
    "                model = XGBClassifier(objective=objective,\n",
    "                                      n_estimators=n_estimators, \n",
    "                                      num_parallel_tree=num_parallel_tree,\n",
    "                                      booster=booster, \n",
    "                                      max_depth=max_depth,\n",
    "                                      learning_rate=learning_rate,\n",
    "                                      gamma=gamma,\n",
    "                                      min_child_weight=min_child_weight,\n",
    "                                      colsample_bytree=colsample_bytree,\n",
    "                                      reg_alpha=reg_alpha,\n",
    "                                      reg_lambda=reg_lambda,\n",
    "                                      max_delta_step=max_delta_step,\n",
    "                                      grow_policy=grow_policy, \n",
    "                                      subsample=subsample, \n",
    "                                      scale_pos_weight=scale_pos_weight, \n",
    "                                      tree_method=tree_method,                                       \n",
    "                                      predictor=predictor,\n",
    "                                      n_jobs=n_jobs\n",
    "                                      )\n",
    "            model.fit(X, Y)\n",
    "            with gzip.open(f'{self.save_dir}/XGB_{booster}_{grow_policy}.pickle','wb') as f:\n",
    "                pickle.dump(model, f)\n",
    "\n",
    "        # Model Load\n",
    "        with gzip.open(f'{self.save_dir}/XGB_{booster}_{grow_policy}.pickle','rb') as f:\n",
    "            model = pickle.load(f)\n",
    "\n",
    "        print(model.get_xgb_params())\n",
    "\n",
    "        score = model.predict_proba(X)[:,1]\n",
    "        if grow_policy == 'depthwise' :\n",
    "            if booster == 'dart' :\n",
    "                score_name = 'DART_S'\n",
    "            elif booster == 'gbtree' :\n",
    "                score_name = 'XGB_S'\n",
    "            else :\n",
    "                score_name = 'XGBL_S'\n",
    "        else :\n",
    "            score_name = 'LGBM_S'\n",
    "\n",
    "        if num_parallel_tree > 1 :\n",
    "            score_name = 'XGBRF_S'\n",
    "\n",
    "        if training == True :\n",
    "            self.xgb_Threshold = 0.5\n",
    "            self.train[score_name] = score\n",
    "            pred = (score >= self.xgb_Threshold).astype('int')\n",
    "            self.train[score_name.replace(\"_S\",\"\")] = pred\n",
    "        else :\n",
    "            self.test[score_name] = score\n",
    "            pred = (score >= self.xgb_Threshold).astype('int')\n",
    "            self.test[score_name.replace(\"_S\",\"\")] = pred\n",
    "\n",
    "        print(f\"Threshold : {self.xgb_Threshold}\")\n",
    "        print(f\"F1-SCORE : {f1_score(Y, pred, average='macro')}\\n\")\n",
    "        print(f\"\\nConfusion Matrix : \\n{confusion_matrix(Y,pred)}\\n\\n\")\n",
    "        print(classification_report(Y, pred))\n",
    "        print(\"=\"*100)  \n",
    "\n",
    "    def auto_run(self, \n",
    "                 X_train, Y_train,\n",
    "                 X_test=None, Y_test=None,\n",
    "                 runList:list=['all']) :\n",
    "\n",
    "        if 'all' in runList :\n",
    "            if X_test is not None :\n",
    "\n",
    "                self.XGBC(X=X_train, Y=Y_train, training=True, booster='gbtree')\n",
    "                self.XGBC(X=X_test, Y=Y_test, training=False, booster='gbtree')\n",
    "\n",
    "                self.XGBC(X=X_train, Y=Y_train, training=True, booster='dart', rate_drop=0.5)\n",
    "                self.XGBC(X=X_test, Y=Y_test, training=False, booster='dart', rate_drop=0.5)\n",
    "\n",
    "                self.XGBC(X=X_train, Y=Y_train, training=True, grow_policy='lossguide')\n",
    "                self.XGBC(X=X_test, Y=Y_test, training=False, grow_policy='lossguide')\n",
    "            else :\n",
    "\n",
    "                self.XGBC(X=X_train, Y=Y_train, training=True, booster='gbtree')\n",
    "                self.XGBC(X=X_train, Y=Y_train, training=True, booster='dart', rate_drop=0.5)\n",
    "                self.XGBC(X=X_train, Y=Y_train, training=True, grow_policy='lossguide')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5224f427-b925-4c99-9349-7c26a50bfebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((262, 35), (175, 35))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"./data/df_train5.csv\")\n",
    "test = pd.read_csv(\"./data/df_test5.csv\")\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d522d8ad-2578-429e-8e59-3866448518a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['class'].values\n",
    "X = train.drop(['id', 'class', 'class_B', 'class_C'], axis=1)\n",
    "X_test = test.drop(['id', 'class', 'class_B', 'class_C'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "196ccd6c-9241-4a35-a847-f0e0450a707c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "작업 경로 : ./results/20221222_142043\n",
      "XGB Classification with gbtree & depthwise is Started!\n",
      "\n",
      "[14:21:08] WARNING: ../src/learner.cc:767: \n",
      "Parameters: { \"scale_pos_weight\" } are not used.\n",
      "\n",
      "{'objective': 'multi:softprob', 'base_score': 0.5, 'booster': 'gbtree', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 1.0, 'eval_metric': None, 'gamma': 0.0, 'gpu_id': 0, 'grow_policy': 'depthwise', 'interaction_constraints': '', 'learning_rate': 0.3, 'max_bin': 256, 'max_cat_threshold': 64, 'max_cat_to_onehot': 4, 'max_delta_step': 0.0, 'max_depth': 6, 'max_leaves': 0, 'min_child_weight': 0.0, 'monotone_constraints': '()', 'n_jobs': -1, 'num_parallel_tree': 1, 'predictor': 'gpu_predictor', 'random_state': 0, 'reg_alpha': 0.0, 'reg_lambda': 1.0, 'sampling_method': 'uniform', 'scale_pos_weight': 1, 'subsample': 1, 'tree_method': 'gpu_hist', 'validate_parameters': 1, 'verbosity': None}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'autoModel' object has no attribute 'optimize_threshold'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m auto \u001b[38;5;241m=\u001b[39m autoModel()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mauto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [16], line 271\u001b[0m, in \u001b[0;36mautoModel.auto_run\u001b[0;34m(self, X_train, Y_train, X_test, Y_test, runList)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mXGBC(X\u001b[38;5;241m=\u001b[39mX_test, Y\u001b[38;5;241m=\u001b[39mY_test, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, grow_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlossguide\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m :\n\u001b[0;32m--> 271\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBC\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbooster\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgbtree\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mXGBC(X\u001b[38;5;241m=\u001b[39mX_train, Y\u001b[38;5;241m=\u001b[39mY_train, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, booster\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdart\u001b[39m\u001b[38;5;124m'\u001b[39m, rate_drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mXGBC(X\u001b[38;5;241m=\u001b[39mX_train, Y\u001b[38;5;241m=\u001b[39mY_train, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, grow_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlossguide\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn [16], line 238\u001b[0m, in \u001b[0;36mautoModel.XGBC\u001b[0;34m(self, X, Y, training, objective, num_parallel_tree, max_depth, learning_rate, gamma, min_child_weight, colsample_bytree, reg_alpha, reg_lambda, max_delta_step, skip_drop, booster, rate_drop, subsample, grow_policy, n_estimators, scale_pos_weight, tree_method, predictor, n_jobs)\u001b[0m\n\u001b[1;32m    235\u001b[0m     score_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXGBRF_S\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m :\n\u001b[0;32m--> 238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxgb_Threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize_threshold\u001b[49m(label\u001b[38;5;241m=\u001b[39mY, score\u001b[38;5;241m=\u001b[39mscore)\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain[score_name] \u001b[38;5;241m=\u001b[39m score\n\u001b[1;32m    240\u001b[0m     pred \u001b[38;5;241m=\u001b[39m (score \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxgb_Threshold)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'autoModel' object has no attribute 'optimize_threshold'"
     ]
    }
   ],
   "source": [
    "auto = autoModel()\n",
    "auto.auto_run(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12d23e1-775e-4f63-87e4-d3c1b2738b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
